{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab on initialization and optimization, let's look at a slightly different type of neural network. This time, we will not perform a classification task as we've done before.  Instead, we'll look at a regression problem.\n",
    "\n",
    "We can just as well use deep learning networks for regression as for a classification problem. However, note that getting regression to work with neural networks is a harder problem because the output is unbounded ($\\hat y$ can technically range from $-\\infty$ to $+\\infty$, and the models are especially prone to **_exploding gradients_**. This issue makes a regression exercise the perfect learning case!\n",
    "\n",
    "Run the cell below to import everything we'll need for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras import initializers\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we'll be working with is data related to facebook posts published during the year of 2014 on the Facebook's page of a renowned cosmetics brand.  It includes 7 features known prior to post publication, and 12 features for evaluating the post impact. What we want to do is make a predictor for the number of \"likes\" for a post, taking into account the 7 features prior to posting.\n",
    "\n",
    "First, let's import the data set and delete any rows with missing data.  \n",
    "\n",
    "The dataset is contained with the file `dataset_Facebook.csv`. In the cell below, use pandas to read in the data from this file. Because of the way the data is structure, make sure you also set the `sep` parameter to `\";\"`, and the `header` parameter to `0`. \n",
    "\n",
    "Then, use the DataFrame's built-in `.dropna()` function to remove any rows with missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('dataset_Facebook.csv', sep=';', header=0)\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the shape of our data to ensure that everything looks correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(495, 19)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data) #Expected Output: (495, 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's inspect the `.head()` of the DataFrame to get a feel for what our dataset looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page total likes</th>\n",
       "      <th>Type</th>\n",
       "      <th>Category</th>\n",
       "      <th>Post Month</th>\n",
       "      <th>Post Weekday</th>\n",
       "      <th>Post Hour</th>\n",
       "      <th>Paid</th>\n",
       "      <th>Lifetime Post Total Reach</th>\n",
       "      <th>Lifetime Post Total Impressions</th>\n",
       "      <th>Lifetime Engaged Users</th>\n",
       "      <th>Lifetime Post Consumers</th>\n",
       "      <th>Lifetime Post Consumptions</th>\n",
       "      <th>Lifetime Post Impressions by people who have liked your Page</th>\n",
       "      <th>Lifetime Post reach by people who like your Page</th>\n",
       "      <th>Lifetime People who have liked your Page and engaged with your post</th>\n",
       "      <th>comment</th>\n",
       "      <th>like</th>\n",
       "      <th>share</th>\n",
       "      <th>Total Interactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2752</td>\n",
       "      <td>5091</td>\n",
       "      <td>178</td>\n",
       "      <td>109</td>\n",
       "      <td>159</td>\n",
       "      <td>3078</td>\n",
       "      <td>1640</td>\n",
       "      <td>119</td>\n",
       "      <td>4</td>\n",
       "      <td>79.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139441</td>\n",
       "      <td>Status</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10460</td>\n",
       "      <td>19057</td>\n",
       "      <td>1457</td>\n",
       "      <td>1361</td>\n",
       "      <td>1674</td>\n",
       "      <td>11710</td>\n",
       "      <td>6112</td>\n",
       "      <td>1108</td>\n",
       "      <td>5</td>\n",
       "      <td>130.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2413</td>\n",
       "      <td>4373</td>\n",
       "      <td>177</td>\n",
       "      <td>113</td>\n",
       "      <td>154</td>\n",
       "      <td>2812</td>\n",
       "      <td>1503</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50128</td>\n",
       "      <td>87991</td>\n",
       "      <td>2211</td>\n",
       "      <td>790</td>\n",
       "      <td>1119</td>\n",
       "      <td>61027</td>\n",
       "      <td>32048</td>\n",
       "      <td>1386</td>\n",
       "      <td>58</td>\n",
       "      <td>1572.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>1777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7244</td>\n",
       "      <td>13594</td>\n",
       "      <td>671</td>\n",
       "      <td>410</td>\n",
       "      <td>580</td>\n",
       "      <td>6228</td>\n",
       "      <td>3200</td>\n",
       "      <td>396</td>\n",
       "      <td>19</td>\n",
       "      <td>325.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Page total likes    Type  Category  Post Month  Post Weekday  Post Hour  \\\n",
       "0            139441   Photo         2          12             4          3   \n",
       "1            139441  Status         2          12             3         10   \n",
       "2            139441   Photo         3          12             3          3   \n",
       "3            139441   Photo         2          12             2         10   \n",
       "4            139441   Photo         2          12             2          3   \n",
       "\n",
       "   Paid  Lifetime Post Total Reach  Lifetime Post Total Impressions  \\\n",
       "0   0.0                       2752                             5091   \n",
       "1   0.0                      10460                            19057   \n",
       "2   0.0                       2413                             4373   \n",
       "3   1.0                      50128                            87991   \n",
       "4   0.0                       7244                            13594   \n",
       "\n",
       "   Lifetime Engaged Users  Lifetime Post Consumers  \\\n",
       "0                     178                      109   \n",
       "1                    1457                     1361   \n",
       "2                     177                      113   \n",
       "3                    2211                      790   \n",
       "4                     671                      410   \n",
       "\n",
       "   Lifetime Post Consumptions  \\\n",
       "0                         159   \n",
       "1                        1674   \n",
       "2                         154   \n",
       "3                        1119   \n",
       "4                         580   \n",
       "\n",
       "   Lifetime Post Impressions by people who have liked your Page  \\\n",
       "0                                               3078              \n",
       "1                                              11710              \n",
       "2                                               2812              \n",
       "3                                              61027              \n",
       "4                                               6228              \n",
       "\n",
       "   Lifetime Post reach by people who like your Page  \\\n",
       "0                                              1640   \n",
       "1                                              6112   \n",
       "2                                              1503   \n",
       "3                                             32048   \n",
       "4                                              3200   \n",
       "\n",
       "   Lifetime People who have liked your Page and engaged with your post  \\\n",
       "0                                                119                     \n",
       "1                                               1108                     \n",
       "2                                                132                     \n",
       "3                                               1386                     \n",
       "4                                                396                     \n",
       "\n",
       "   comment    like  share  Total Interactions  \n",
       "0        4    79.0   17.0                 100  \n",
       "1        5   130.0   29.0                 164  \n",
       "2        0    66.0   14.0                  80  \n",
       "3       58  1572.0  147.0                1777  \n",
       "4       19   325.0   49.0                 393  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Normalize the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big part of Deep Learning is cleaning the data and getting into a shape usable by a neural network.  Let's get some additional practice with this.\n",
    "\n",
    "\n",
    "Take a look at our input data. We'll use the 7 first columns as our predictors. We'll do the following two things:\n",
    "- Normalize the continuous variables --> you can do this using `np.mean()` and `np.std()`\n",
    "- make dummy variables of the categorical variables (you can do this by using `pd.get_dummies`)\n",
    "\n",
    "We only count \"Category\" and \"Type\" as categorical variables. Note that you can argue that \"Post month\", \"Post Weekday\" and \"Post Hour\" can also be considered categories, but we'll just treat them as being continuous for now.\n",
    "\n",
    "In the cell below, convert the data as needed by normalizing or converting to dummy variables, and then concatenate it all back into a single DataFrame once you've finished.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = data[\"Page total likes\"]\n",
    "X1 = data[\"Type\"]\n",
    "X2 = data[\"Category\"]\n",
    "X3 = data[\"Post Month\"]\n",
    "X4 = data[\"Post Weekday\"]\n",
    "X5 = data[\"Post Hour\"]\n",
    "X6 = data[\"Paid\"]\n",
    "\n",
    "## standardize/categorize\n",
    "X0= (X0-np.mean(X0))/np.std(X0)\n",
    "dummy_X1= pd.get_dummies(X1)\n",
    "dummy_X2= pd.get_dummies(X2)\n",
    "X3= (X3-np.mean(X3))/np.std(X3)\n",
    "X4= (X4-np.mean(X4))/np.std(X4)\n",
    "X5= (X5-np.mean(X5))/np.std(X5)\n",
    "\n",
    "# Add them all back into a single DataFrame\n",
    "X = pd.concat([X0, dummy_X1, dummy_X2, X3, X4, X5, X6], axis=1)\n",
    "\n",
    "# Store our labels in a separate variable\n",
    "Y = data[\"like\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: you get the same result for standardization if you use StandardScaler from sklearn.preprocessing\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler()\n",
    "# X0 = sc.fit_transform(X0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is fairly small. Let's just split the data up in a training set and a validation set!\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Split the data into training and testing sets by passing `X` and `Y` into `train_test_split`.  Set a `test_size` of `0.2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shape to make sure everything worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 12)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape # Expected Output: (99, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(396, 12)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape # Expected Output: (396, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network for Regression\n",
    "\n",
    "Now, we'll build a neural network to predict the number of likes we think a post will receive.  \n",
    "\n",
    "In the cell below, create a model with the following specifications:\n",
    "\n",
    "* 1 Hidden Layer with 8 neurons.  In this layer, also set `input_dim` to `12`, and `activation` to `\"relu\"`.\n",
    "* An output layer with 1 neuron.  For this neuron, set the activation to `linear`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(8, activation='relu', input_dim=12))\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to compile the model, with the following hyperparameters:\n",
    "\n",
    "* `optimizer='sgd'`\n",
    "* `loss='mse'`\n",
    "* `metrics=['mse']`\n",
    "\n",
    "Note that since our model is training for a regression task, not a classification task, we'll need to use a loss metric that corresponds with regression tasks--Mean Squared Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD', loss='MSE', metrics=['MSE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's train the model.  Call `model.fit()`. In addition to to the training data and labels, also set:\n",
    "\n",
    "* `batch_size=32`\n",
    "* `epochs=100`\n",
    "* `verbose=1`\n",
    "* `validation_data=(X_val, y_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 396 samples, validate on 99 samples\n",
      "Epoch 1/100\n",
      "396/396 [==============================] - 0s 519us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 2/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 3/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 4/100\n",
      "396/396 [==============================] - 0s 89us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 5/100\n",
      "396/396 [==============================] - 0s 64us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 6/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 7/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 8/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 9/100\n",
      "396/396 [==============================] - 0s 90us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 10/100\n",
      "396/396 [==============================] - 0s 67us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 11/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 12/100\n",
      "396/396 [==============================] - 0s 74us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 13/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 14/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 15/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 16/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 17/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 18/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 19/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 20/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 21/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 22/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 23/100\n",
      "396/396 [==============================] - 0s 59us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 24/100\n",
      "396/396 [==============================] - 0s 84us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 25/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 26/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 27/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 28/100\n",
      "396/396 [==============================] - 0s 69us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 29/100\n",
      "396/396 [==============================] - 0s 62us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 30/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 31/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 32/100\n",
      "396/396 [==============================] - 0s 79us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 33/100\n",
      "396/396 [==============================] - 0s 96us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 34/100\n",
      "396/396 [==============================] - 0s 42us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 35/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 36/100\n",
      "396/396 [==============================] - 0s 56us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 37/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 38/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 39/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 40/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 41/100\n",
      "396/396 [==============================] - 0s 44us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 42/100\n",
      "396/396 [==============================] - 0s 46us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 43/100\n",
      "396/396 [==============================] - 0s 173us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 44/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 45/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 46/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 47/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 48/100\n",
      "396/396 [==============================] - 0s 44us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 49/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 50/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 51/100\n",
      "396/396 [==============================] - 0s 43us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 52/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 53/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s 54us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 55/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 56/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 57/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 58/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 59/100\n",
      "396/396 [==============================] - 0s 45us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 60/100\n",
      "396/396 [==============================] - 0s 44us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 61/100\n",
      "396/396 [==============================] - 0s 44us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 62/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 63/100\n",
      "396/396 [==============================] - 0s 56us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 64/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 65/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 66/100\n",
      "396/396 [==============================] - 0s 103us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 67/100\n",
      "396/396 [==============================] - 0s 114us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 68/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 69/100\n",
      "396/396 [==============================] - 0s 79us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 70/100\n",
      "396/396 [==============================] - 0s 80us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 71/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 72/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 73/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 74/100\n",
      "396/396 [==============================] - 0s 69us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 75/100\n",
      "396/396 [==============================] - 0s 62us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 76/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 77/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 78/100\n",
      "396/396 [==============================] - 0s 70us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 79/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 80/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 81/100\n",
      "396/396 [==============================] - 0s 80us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 82/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 83/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 84/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 85/100\n",
      "396/396 [==============================] - 0s 59us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 86/100\n",
      "396/396 [==============================] - 0s 79us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 87/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 88/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 89/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 90/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 91/100\n",
      "396/396 [==============================] - 0s 72us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 92/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 93/100\n",
      "396/396 [==============================] - 0s 46us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 94/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 95/100\n",
      "396/396 [==============================] - 0s 212us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 96/100\n",
      "396/396 [==============================] - 0s 233us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 97/100\n",
      "396/396 [==============================] - 0s 98us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 98/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 99/100\n",
      "396/396 [==============================] - 0s 69us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 100/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train, batch_size=32, epochs=100,\n",
    "                verbose=1, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you see what happend? all the values for training and validation loss are \"nan\". There could be several reasons for that, but as we already mentioned there is likely a vanishing or exploding gradient problem.  This means that the values got so large or so small that they no longer fit in memory.   R\n",
    "\n",
    "Recall that we normalized out inputs. But how about the outputs? Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212     36.0\n",
       "107    193.0\n",
       "411     75.0\n",
       "71     449.0\n",
       "473    136.0\n",
       "Name: like, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, indeed. We didn't normalize them and we should, as they take pretty high values. Let\n",
    "s rerun the model but make sure that the output is normalized as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Normalizing the output\n",
    "\n",
    "In the cell below, we've included all the normalization code that we wrote up top, but this time, we've added a line to normalize the data in `Y`, as well. This should help alot!\n",
    "\n",
    "Run the cell below to normalize our data and our labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = data[\"Page total likes\"]\n",
    "X1 = data[\"Type\"]\n",
    "X2 = data[\"Category\"]\n",
    "X3 = data[\"Post Month\"]\n",
    "X4 = data[\"Post Weekday\"]\n",
    "X5 = data[\"Post Hour\"]\n",
    "X6 = data[\"Paid\"]\n",
    "\n",
    "## standardize/categorize\n",
    "X0= (X0-np.mean(X0))/(np.std(X0))\n",
    "dummy_X1= pd.get_dummies(X1)\n",
    "dummy_X2= pd.get_dummies(X2)\n",
    "X3= (X3-np.mean(X3))/(np.std(X3))\n",
    "X4= (X4-np.mean(X4))/(np.std(X4))\n",
    "X5= (X5-np.mean(X5))/(np.std(X5))\n",
    "\n",
    "X = pd.concat([X0, dummy_X1, dummy_X2, X3, X4, X5, X6], axis=1)\n",
    "\n",
    "Y = (data[\"like\"]-np.mean(data[\"like\"]))/(np.std(data[\"like\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split our data into appropriate training and testing sets again.  Split the data, just like we did before.  Use the same `test_size` as we did last time, too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's reinitialize our model and build it from scratch again.  \n",
    "\n",
    "**_NOTE:_**  If we don't reinitialize our model, our training would start with the weight values we ended with during the last training session.  In order to start fresh, we need to declare a new `Sequential()` object.  \n",
    "\n",
    "Build the model with the exact same architecture and hyperparameters as we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(8, activation='relu', input_dim=12))\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compile the model with the same parameters we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD', loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, fit the model using the same parameters we did before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 396 samples, validate on 99 samples\n",
      "Epoch 1/100\n",
      "396/396 [==============================] - 0s 516us/step - loss: 1.7555 - mean_squared_error: 1.7555 - val_loss: 0.3350 - val_mean_squared_error: 0.3350\n",
      "Epoch 2/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: 1.3417 - mean_squared_error: 1.3417 - val_loss: 0.3002 - val_mean_squared_error: 0.3002\n",
      "Epoch 3/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.2700 - mean_squared_error: 1.2700 - val_loss: 0.3005 - val_mean_squared_error: 0.3005\n",
      "Epoch 4/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 1.2396 - mean_squared_error: 1.2396 - val_loss: 0.2923 - val_mean_squared_error: 0.2923\n",
      "Epoch 5/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 1.2186 - mean_squared_error: 1.2186 - val_loss: 0.2870 - val_mean_squared_error: 0.2870\n",
      "Epoch 6/100\n",
      "396/396 [==============================] - 0s 96us/step - loss: 1.2036 - mean_squared_error: 1.2036 - val_loss: 0.2840 - val_mean_squared_error: 0.2840\n",
      "Epoch 7/100\n",
      "396/396 [==============================] - 0s 79us/step - loss: 1.1943 - mean_squared_error: 1.1943 - val_loss: 0.2735 - val_mean_squared_error: 0.2735\n",
      "Epoch 8/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.1837 - mean_squared_error: 1.1837 - val_loss: 0.2707 - val_mean_squared_error: 0.2707\n",
      "Epoch 9/100\n",
      "396/396 [==============================] - 0s 82us/step - loss: 1.1728 - mean_squared_error: 1.1728 - val_loss: 0.2823 - val_mean_squared_error: 0.2823\n",
      "Epoch 10/100\n",
      "396/396 [==============================] - 0s 109us/step - loss: 1.1646 - mean_squared_error: 1.1646 - val_loss: 0.2892 - val_mean_squared_error: 0.2892\n",
      "Epoch 11/100\n",
      "396/396 [==============================] - 0s 43us/step - loss: 1.1563 - mean_squared_error: 1.1563 - val_loss: 0.2702 - val_mean_squared_error: 0.2702\n",
      "Epoch 12/100\n",
      "396/396 [==============================] - 0s 56us/step - loss: 1.1516 - mean_squared_error: 1.1516 - val_loss: 0.2763 - val_mean_squared_error: 0.2763\n",
      "Epoch 13/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 1.1463 - mean_squared_error: 1.1463 - val_loss: 0.2628 - val_mean_squared_error: 0.2628\n",
      "Epoch 14/100\n",
      "396/396 [==============================] - 0s 72us/step - loss: 1.1423 - mean_squared_error: 1.1423 - val_loss: 0.2687 - val_mean_squared_error: 0.2687\n",
      "Epoch 15/100\n",
      "396/396 [==============================] - 0s 46us/step - loss: 1.1383 - mean_squared_error: 1.1383 - val_loss: 0.2532 - val_mean_squared_error: 0.2532\n",
      "Epoch 16/100\n",
      "396/396 [==============================] - 0s 46us/step - loss: 1.1369 - mean_squared_error: 1.1369 - val_loss: 0.2582 - val_mean_squared_error: 0.2582\n",
      "Epoch 17/100\n",
      "396/396 [==============================] - 0s 74us/step - loss: 1.1348 - mean_squared_error: 1.1348 - val_loss: 0.2619 - val_mean_squared_error: 0.2619\n",
      "Epoch 18/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.1327 - mean_squared_error: 1.1327 - val_loss: 0.2658 - val_mean_squared_error: 0.2658\n",
      "Epoch 19/100\n",
      "396/396 [==============================] - 0s 87us/step - loss: 1.1301 - mean_squared_error: 1.1301 - val_loss: 0.2805 - val_mean_squared_error: 0.2805\n",
      "Epoch 20/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 1.1281 - mean_squared_error: 1.1281 - val_loss: 0.2628 - val_mean_squared_error: 0.2628\n",
      "Epoch 21/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.1264 - mean_squared_error: 1.1264 - val_loss: 0.2556 - val_mean_squared_error: 0.2556\n",
      "Epoch 22/100\n",
      "396/396 [==============================] - 0s 101us/step - loss: 1.1236 - mean_squared_error: 1.1236 - val_loss: 0.2593 - val_mean_squared_error: 0.2593\n",
      "Epoch 23/100\n",
      "396/396 [==============================] - 0s 96us/step - loss: 1.1201 - mean_squared_error: 1.1201 - val_loss: 0.2482 - val_mean_squared_error: 0.2482\n",
      "Epoch 24/100\n",
      "396/396 [==============================] - 0s 101us/step - loss: 1.1209 - mean_squared_error: 1.1209 - val_loss: 0.2551 - val_mean_squared_error: 0.2551\n",
      "Epoch 25/100\n",
      "396/396 [==============================] - 0s 74us/step - loss: 1.1186 - mean_squared_error: 1.1186 - val_loss: 0.2648 - val_mean_squared_error: 0.2648\n",
      "Epoch 26/100\n",
      "396/396 [==============================] - 0s 84us/step - loss: 1.1155 - mean_squared_error: 1.1155 - val_loss: 0.2645 - val_mean_squared_error: 0.2645\n",
      "Epoch 27/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 1.1133 - mean_squared_error: 1.1133 - val_loss: 0.2842 - val_mean_squared_error: 0.2842\n",
      "Epoch 28/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: 1.1116 - mean_squared_error: 1.1116 - val_loss: 0.2578 - val_mean_squared_error: 0.2578\n",
      "Epoch 29/100\n",
      "396/396 [==============================] - 0s 44us/step - loss: 1.1108 - mean_squared_error: 1.1108 - val_loss: 0.2585 - val_mean_squared_error: 0.2585\n",
      "Epoch 30/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: 1.1102 - mean_squared_error: 1.1102 - val_loss: 0.2618 - val_mean_squared_error: 0.2618\n",
      "Epoch 31/100\n",
      "396/396 [==============================] - 0s 46us/step - loss: 1.1092 - mean_squared_error: 1.1092 - val_loss: 0.2674 - val_mean_squared_error: 0.2674\n",
      "Epoch 32/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 1.1084 - mean_squared_error: 1.1084 - val_loss: 0.2685 - val_mean_squared_error: 0.2685\n",
      "Epoch 33/100\n",
      "396/396 [==============================] - 0s 102us/step - loss: 1.1072 - mean_squared_error: 1.1072 - val_loss: 0.2605 - val_mean_squared_error: 0.2605\n",
      "Epoch 34/100\n",
      "396/396 [==============================] - 0s 79us/step - loss: 1.1063 - mean_squared_error: 1.1063 - val_loss: 0.2573 - val_mean_squared_error: 0.2573\n",
      "Epoch 35/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: 1.1081 - mean_squared_error: 1.1081 - val_loss: 0.2544 - val_mean_squared_error: 0.2544\n",
      "Epoch 36/100\n",
      "396/396 [==============================] - 0s 67us/step - loss: 1.1050 - mean_squared_error: 1.1050 - val_loss: 0.2606 - val_mean_squared_error: 0.2606\n",
      "Epoch 37/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.1019 - mean_squared_error: 1.1019 - val_loss: 0.2693 - val_mean_squared_error: 0.2693\n",
      "Epoch 38/100\n",
      "396/396 [==============================] - 0s 104us/step - loss: 1.1021 - mean_squared_error: 1.1021 - val_loss: 0.2647 - val_mean_squared_error: 0.2647\n",
      "Epoch 39/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 1.1008 - mean_squared_error: 1.1008 - val_loss: 0.2689 - val_mean_squared_error: 0.2689\n",
      "Epoch 40/100\n",
      "396/396 [==============================] - 0s 110us/step - loss: 1.0984 - mean_squared_error: 1.0984 - val_loss: 0.2885 - val_mean_squared_error: 0.2885\n",
      "Epoch 41/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.0992 - mean_squared_error: 1.0992 - val_loss: 0.2639 - val_mean_squared_error: 0.2639\n",
      "Epoch 42/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.0977 - mean_squared_error: 1.0977 - val_loss: 0.2554 - val_mean_squared_error: 0.2554\n",
      "Epoch 43/100\n",
      "396/396 [==============================] - 0s 92us/step - loss: 1.0976 - mean_squared_error: 1.0976 - val_loss: 0.2565 - val_mean_squared_error: 0.2565\n",
      "Epoch 44/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.0965 - mean_squared_error: 1.0965 - val_loss: 0.2592 - val_mean_squared_error: 0.2592\n",
      "Epoch 45/100\n",
      "396/396 [==============================] - 0s 46us/step - loss: 1.0932 - mean_squared_error: 1.0932 - val_loss: 0.2610 - val_mean_squared_error: 0.2610\n",
      "Epoch 46/100\n",
      "396/396 [==============================] - 0s 46us/step - loss: 1.0921 - mean_squared_error: 1.0921 - val_loss: 0.2614 - val_mean_squared_error: 0.2614\n",
      "Epoch 47/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: 1.0919 - mean_squared_error: 1.0919 - val_loss: 0.2625 - val_mean_squared_error: 0.2625\n",
      "Epoch 48/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: 1.0913 - mean_squared_error: 1.0913 - val_loss: 0.2557 - val_mean_squared_error: 0.2557\n",
      "Epoch 49/100\n",
      "396/396 [==============================] - 0s 46us/step - loss: 1.0924 - mean_squared_error: 1.0924 - val_loss: 0.2521 - val_mean_squared_error: 0.2521\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s 46us/step - loss: 1.0906 - mean_squared_error: 1.0906 - val_loss: 0.2579 - val_mean_squared_error: 0.2579\n",
      "Epoch 51/100\n",
      "396/396 [==============================] - 0s 45us/step - loss: 1.0906 - mean_squared_error: 1.0906 - val_loss: 0.2595 - val_mean_squared_error: 0.2595\n",
      "Epoch 52/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.0878 - mean_squared_error: 1.0878 - val_loss: 0.2539 - val_mean_squared_error: 0.2539\n",
      "Epoch 53/100\n",
      "396/396 [==============================] - 0s 87us/step - loss: 1.0873 - mean_squared_error: 1.0873 - val_loss: 0.2488 - val_mean_squared_error: 0.2488\n",
      "Epoch 54/100\n",
      "396/396 [==============================] - 0s 91us/step - loss: 1.0885 - mean_squared_error: 1.0885 - val_loss: 0.2562 - val_mean_squared_error: 0.2562\n",
      "Epoch 55/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: 1.0860 - mean_squared_error: 1.0860 - val_loss: 0.2616 - val_mean_squared_error: 0.2616\n",
      "Epoch 56/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.0858 - mean_squared_error: 1.0858 - val_loss: 0.2821 - val_mean_squared_error: 0.2821\n",
      "Epoch 57/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: 1.0830 - mean_squared_error: 1.0830 - val_loss: 0.2785 - val_mean_squared_error: 0.2785\n",
      "Epoch 58/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 1.0822 - mean_squared_error: 1.0822 - val_loss: 0.2727 - val_mean_squared_error: 0.2727\n",
      "Epoch 59/100\n",
      "396/396 [==============================] - 0s 94us/step - loss: 1.0827 - mean_squared_error: 1.0827 - val_loss: 0.2888 - val_mean_squared_error: 0.2888\n",
      "Epoch 60/100\n",
      "396/396 [==============================] - 0s 79us/step - loss: 1.0832 - mean_squared_error: 1.0832 - val_loss: 0.2748 - val_mean_squared_error: 0.2748\n",
      "Epoch 61/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.0809 - mean_squared_error: 1.0809 - val_loss: 0.2647 - val_mean_squared_error: 0.2647\n",
      "Epoch 62/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 1.0820 - mean_squared_error: 1.0820 - val_loss: 0.2644 - val_mean_squared_error: 0.2644\n",
      "Epoch 63/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 1.0801 - mean_squared_error: 1.0801 - val_loss: 0.2713 - val_mean_squared_error: 0.2713\n",
      "Epoch 64/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 1.0797 - mean_squared_error: 1.0797 - val_loss: 0.2644 - val_mean_squared_error: 0.2644\n",
      "Epoch 65/100\n",
      "396/396 [==============================] - 0s 104us/step - loss: 1.0778 - mean_squared_error: 1.0778 - val_loss: 0.2746 - val_mean_squared_error: 0.2746\n",
      "Epoch 66/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 1.0794 - mean_squared_error: 1.0794 - val_loss: 0.2629 - val_mean_squared_error: 0.2629\n",
      "Epoch 67/100\n",
      "396/396 [==============================] - 0s 70us/step - loss: 1.0768 - mean_squared_error: 1.0768 - val_loss: 0.2589 - val_mean_squared_error: 0.2589\n",
      "Epoch 68/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: 1.0776 - mean_squared_error: 1.0776 - val_loss: 0.2847 - val_mean_squared_error: 0.2847\n",
      "Epoch 69/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 1.0770 - mean_squared_error: 1.0770 - val_loss: 0.2749 - val_mean_squared_error: 0.2749\n",
      "Epoch 70/100\n",
      "396/396 [==============================] - 0s 70us/step - loss: 1.0742 - mean_squared_error: 1.0742 - val_loss: 0.2791 - val_mean_squared_error: 0.2791\n",
      "Epoch 71/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 1.0749 - mean_squared_error: 1.0749 - val_loss: 0.2659 - val_mean_squared_error: 0.2659\n",
      "Epoch 72/100\n",
      "396/396 [==============================] - 0s 67us/step - loss: 1.0739 - mean_squared_error: 1.0739 - val_loss: 0.2732 - val_mean_squared_error: 0.2732\n",
      "Epoch 73/100\n",
      "396/396 [==============================] - 0s 130us/step - loss: 1.0718 - mean_squared_error: 1.0718 - val_loss: 0.2750 - val_mean_squared_error: 0.2750\n",
      "Epoch 74/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 1.0727 - mean_squared_error: 1.0727 - val_loss: 0.3117 - val_mean_squared_error: 0.3117\n",
      "Epoch 75/100\n",
      "396/396 [==============================] - 0s 140us/step - loss: 1.0756 - mean_squared_error: 1.0756 - val_loss: 0.2954 - val_mean_squared_error: 0.2954\n",
      "Epoch 76/100\n",
      "396/396 [==============================] - 0s 118us/step - loss: 1.0749 - mean_squared_error: 1.0749 - val_loss: 0.2812 - val_mean_squared_error: 0.2812\n",
      "Epoch 77/100\n",
      "396/396 [==============================] - 0s 108us/step - loss: 1.0721 - mean_squared_error: 1.0721 - val_loss: 0.2818 - val_mean_squared_error: 0.2818\n",
      "Epoch 78/100\n",
      "396/396 [==============================] - 0s 91us/step - loss: 1.0725 - mean_squared_error: 1.0725 - val_loss: 0.2711 - val_mean_squared_error: 0.2711\n",
      "Epoch 79/100\n",
      "396/396 [==============================] - 0s 88us/step - loss: 1.0702 - mean_squared_error: 1.0702 - val_loss: 0.2742 - val_mean_squared_error: 0.2742\n",
      "Epoch 80/100\n",
      "396/396 [==============================] - 0s 99us/step - loss: 1.0713 - mean_squared_error: 1.0713 - val_loss: 0.2944 - val_mean_squared_error: 0.2944\n",
      "Epoch 81/100\n",
      "396/396 [==============================] - 0s 98us/step - loss: 1.0697 - mean_squared_error: 1.0697 - val_loss: 0.2836 - val_mean_squared_error: 0.2836\n",
      "Epoch 82/100\n",
      "396/396 [==============================] - 0s 90us/step - loss: 1.0689 - mean_squared_error: 1.0689 - val_loss: 0.2781 - val_mean_squared_error: 0.2781\n",
      "Epoch 83/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: 1.0686 - mean_squared_error: 1.0686 - val_loss: 0.2708 - val_mean_squared_error: 0.2708\n",
      "Epoch 84/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: 1.0672 - mean_squared_error: 1.0672 - val_loss: 0.2660 - val_mean_squared_error: 0.2660\n",
      "Epoch 85/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: 1.0683 - mean_squared_error: 1.0683 - val_loss: 0.2628 - val_mean_squared_error: 0.2628\n",
      "Epoch 86/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.0657 - mean_squared_error: 1.0657 - val_loss: 0.2699 - val_mean_squared_error: 0.2699\n",
      "Epoch 87/100\n",
      "396/396 [==============================] - 0s 88us/step - loss: 1.0649 - mean_squared_error: 1.0649 - val_loss: 0.2674 - val_mean_squared_error: 0.2674\n",
      "Epoch 88/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 1.0646 - mean_squared_error: 1.0646 - val_loss: 0.3036 - val_mean_squared_error: 0.3036\n",
      "Epoch 89/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.0671 - mean_squared_error: 1.0671 - val_loss: 0.2798 - val_mean_squared_error: 0.2798\n",
      "Epoch 90/100\n",
      "396/396 [==============================] - 0s 99us/step - loss: 1.0642 - mean_squared_error: 1.0642 - val_loss: 0.2718 - val_mean_squared_error: 0.2718\n",
      "Epoch 91/100\n",
      "396/396 [==============================] - 0s 148us/step - loss: 1.0650 - mean_squared_error: 1.0650 - val_loss: 0.2845 - val_mean_squared_error: 0.2845\n",
      "Epoch 92/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 1.0630 - mean_squared_error: 1.0630 - val_loss: 0.2733 - val_mean_squared_error: 0.2733\n",
      "Epoch 93/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.0630 - mean_squared_error: 1.0630 - val_loss: 0.2816 - val_mean_squared_error: 0.2816\n",
      "Epoch 94/100\n",
      "396/396 [==============================] - 0s 45us/step - loss: 1.0613 - mean_squared_error: 1.0613 - val_loss: 0.2887 - val_mean_squared_error: 0.2887\n",
      "Epoch 95/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: 1.0600 - mean_squared_error: 1.0600 - val_loss: 0.2763 - val_mean_squared_error: 0.2763\n",
      "Epoch 96/100\n",
      "396/396 [==============================] - 0s 90us/step - loss: 1.0611 - mean_squared_error: 1.0611 - val_loss: 0.3136 - val_mean_squared_error: 0.3136\n",
      "Epoch 97/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 1.0615 - mean_squared_error: 1.0615 - val_loss: 0.2799 - val_mean_squared_error: 0.2799\n",
      "Epoch 98/100\n",
      "396/396 [==============================] - 0s 46us/step - loss: 1.0593 - mean_squared_error: 1.0593 - val_loss: 0.2794 - val_mean_squared_error: 0.2794\n",
      "Epoch 99/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: 1.0575 - mean_squared_error: 1.0575 - val_loss: 0.2816 - val_mean_squared_error: 0.2816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      "396/396 [==============================] - 0s 75us/step - loss: 1.0576 - mean_squared_error: 1.0576 - val_loss: 0.2832 - val_mean_squared_error: 0.2832\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train, batch_size=32, epochs=100,\n",
    "                verbose=1, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model did much, much better this time around!\n",
    "\n",
    "Run the cell below to get the model's predictions for both the training and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first 10 predictions from `pred_train`. Display those in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1336047 , -0.48186985, -0.30847377,  0.5457481 , -0.0837617 ,\n",
       "       -0.05147585,  0.10491547,  0.10348544, -0.4340665 , -0.06438229],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's manually calculate the Mean Squared Error in the cell below.  \n",
    "\n",
    "As a refresher, here's the formula for calculating Mean Squared Error:\n",
    "\n",
    "<img src='mse_formula.gif'>\n",
    "\n",
    "Use `pred_train` and `Y_train` to calculate our training MSE in the cell below.  \n",
    "\n",
    "**_HINT:_** Use numpy to make short work of this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0534526639414505"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_train = np.sum(np.power((pred_train-Y_train),2))/len(pred_train)\n",
    "MSE_train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, calculate the MSE for our validation set in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28320759811935026"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_val = np.mean((pred_val-Y_val)**2)\n",
    "MSE_val "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Use weight initializers\n",
    "\n",
    "Another way to increase the performance of our models is to initialize our weights in clever ways.  We'll explore some of those options below.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1  He initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and use a weight initializer.  We'll start with the **_He normalizer_**, which initializes the weight vector to have an average 0 and a variance of 2/n, with $n$ the number of features feeding into a layer.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Recreate the Neural Network that we created above.  This time, in the hidden layer, set the `kernel_initializer` to `\"he_normal\"`.\n",
    "* Compile and fit the model with the same hyperparameters as we used before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 396 samples, validate on 99 samples\n",
      "Epoch 1/100\n",
      "396/396 [==============================] - 0s 556us/step - loss: 3.0270 - mean_squared_error: 3.0270 - val_loss: 0.8771 - val_mean_squared_error: 0.8771\n",
      "Epoch 2/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.7447 - mean_squared_error: 1.7447 - val_loss: 0.5735 - val_mean_squared_error: 0.5735\n",
      "Epoch 3/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 1.4373 - mean_squared_error: 1.4373 - val_loss: 0.4822 - val_mean_squared_error: 0.4822\n",
      "Epoch 4/100\n",
      "396/396 [==============================] - 0s 72us/step - loss: 1.3241 - mean_squared_error: 1.3241 - val_loss: 0.3776 - val_mean_squared_error: 0.3776\n",
      "Epoch 5/100\n",
      "396/396 [==============================] - 0s 82us/step - loss: 1.2773 - mean_squared_error: 1.2773 - val_loss: 0.3714 - val_mean_squared_error: 0.3714\n",
      "Epoch 6/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 1.2469 - mean_squared_error: 1.2469 - val_loss: 0.3290 - val_mean_squared_error: 0.3290\n",
      "Epoch 7/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.2306 - mean_squared_error: 1.2306 - val_loss: 0.3197 - val_mean_squared_error: 0.3197\n",
      "Epoch 8/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 1.2154 - mean_squared_error: 1.2154 - val_loss: 0.3160 - val_mean_squared_error: 0.3160\n",
      "Epoch 9/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 1.2019 - mean_squared_error: 1.2019 - val_loss: 0.3069 - val_mean_squared_error: 0.3069\n",
      "Epoch 10/100\n",
      "396/396 [==============================] - 0s 46us/step - loss: 1.1920 - mean_squared_error: 1.1920 - val_loss: 0.3020 - val_mean_squared_error: 0.3020\n",
      "Epoch 11/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 1.1895 - mean_squared_error: 1.1895 - val_loss: 0.2905 - val_mean_squared_error: 0.2905\n",
      "Epoch 12/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 1.1849 - mean_squared_error: 1.1849 - val_loss: 0.2888 - val_mean_squared_error: 0.2888\n",
      "Epoch 13/100\n",
      "396/396 [==============================] - 0s 64us/step - loss: 1.1771 - mean_squared_error: 1.1771 - val_loss: 0.2918 - val_mean_squared_error: 0.2918\n",
      "Epoch 14/100\n",
      "396/396 [==============================] - 0s 46us/step - loss: 1.1726 - mean_squared_error: 1.1726 - val_loss: 0.2920 - val_mean_squared_error: 0.2920\n",
      "Epoch 15/100\n",
      "396/396 [==============================] - 0s 72us/step - loss: 1.1692 - mean_squared_error: 1.1692 - val_loss: 0.3103 - val_mean_squared_error: 0.3103\n",
      "Epoch 16/100\n",
      "396/396 [==============================] - 0s 64us/step - loss: 1.1666 - mean_squared_error: 1.1666 - val_loss: 0.2918 - val_mean_squared_error: 0.2918\n",
      "Epoch 17/100\n",
      "396/396 [==============================] - 0s 44us/step - loss: 1.1605 - mean_squared_error: 1.1605 - val_loss: 0.3118 - val_mean_squared_error: 0.3118\n",
      "Epoch 18/100\n",
      "396/396 [==============================] - 0s 42us/step - loss: 1.1567 - mean_squared_error: 1.1567 - val_loss: 0.2804 - val_mean_squared_error: 0.2804\n",
      "Epoch 19/100\n",
      "396/396 [==============================] - 0s 64us/step - loss: 1.1550 - mean_squared_error: 1.1550 - val_loss: 0.2748 - val_mean_squared_error: 0.2748\n",
      "Epoch 20/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 1.1553 - mean_squared_error: 1.1553 - val_loss: 0.3119 - val_mean_squared_error: 0.3119\n",
      "Epoch 21/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.1525 - mean_squared_error: 1.1525 - val_loss: 0.3132 - val_mean_squared_error: 0.3132\n",
      "Epoch 22/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 1.1490 - mean_squared_error: 1.1490 - val_loss: 0.3111 - val_mean_squared_error: 0.3111\n",
      "Epoch 23/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 1.1453 - mean_squared_error: 1.1453 - val_loss: 0.3208 - val_mean_squared_error: 0.3208\n",
      "Epoch 24/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 1.1493 - mean_squared_error: 1.1493 - val_loss: 0.3049 - val_mean_squared_error: 0.3049\n",
      "Epoch 25/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: 1.1451 - mean_squared_error: 1.1451 - val_loss: 0.2968 - val_mean_squared_error: 0.2968\n",
      "Epoch 26/100\n",
      "396/396 [==============================] - 0s 70us/step - loss: 1.1438 - mean_squared_error: 1.1438 - val_loss: 0.2959 - val_mean_squared_error: 0.2959\n",
      "Epoch 27/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 1.1419 - mean_squared_error: 1.1419 - val_loss: 0.3437 - val_mean_squared_error: 0.3437\n",
      "Epoch 28/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: 1.1399 - mean_squared_error: 1.1399 - val_loss: 0.2942 - val_mean_squared_error: 0.2942\n",
      "Epoch 29/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: 1.1382 - mean_squared_error: 1.1382 - val_loss: 0.2826 - val_mean_squared_error: 0.2826\n",
      "Epoch 30/100\n",
      "396/396 [==============================] - 0s 44us/step - loss: 1.1396 - mean_squared_error: 1.1396 - val_loss: 0.2833 - val_mean_squared_error: 0.2833\n",
      "Epoch 31/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 1.1381 - mean_squared_error: 1.1381 - val_loss: 0.2899 - val_mean_squared_error: 0.2899\n",
      "Epoch 32/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 1.1365 - mean_squared_error: 1.1365 - val_loss: 0.2942 - val_mean_squared_error: 0.2942\n",
      "Epoch 33/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: 1.1353 - mean_squared_error: 1.1353 - val_loss: 0.2835 - val_mean_squared_error: 0.2835\n",
      "Epoch 34/100\n",
      "396/396 [==============================] - 0s 45us/step - loss: 1.1376 - mean_squared_error: 1.1376 - val_loss: 0.2875 - val_mean_squared_error: 0.2875\n",
      "Epoch 35/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 1.1333 - mean_squared_error: 1.1333 - val_loss: 0.2854 - val_mean_squared_error: 0.2854\n",
      "Epoch 36/100\n",
      "396/396 [==============================] - 0s 44us/step - loss: 1.1338 - mean_squared_error: 1.1338 - val_loss: 0.2949 - val_mean_squared_error: 0.2949\n",
      "Epoch 37/100\n",
      "396/396 [==============================] - 0s 45us/step - loss: 1.1324 - mean_squared_error: 1.1324 - val_loss: 0.2737 - val_mean_squared_error: 0.2737\n",
      "Epoch 38/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 1.1340 - mean_squared_error: 1.1340 - val_loss: 0.3157 - val_mean_squared_error: 0.3157\n",
      "Epoch 39/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 1.1338 - mean_squared_error: 1.1338 - val_loss: 0.2912 - val_mean_squared_error: 0.2912\n",
      "Epoch 40/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: 1.1288 - mean_squared_error: 1.1288 - val_loss: 0.2877 - val_mean_squared_error: 0.2877\n",
      "Epoch 41/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.1302 - mean_squared_error: 1.1302 - val_loss: 0.2800 - val_mean_squared_error: 0.2800\n",
      "Epoch 42/100\n",
      "396/396 [==============================] - 0s 157us/step - loss: 1.1294 - mean_squared_error: 1.1294 - val_loss: 0.2854 - val_mean_squared_error: 0.2854\n",
      "Epoch 43/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.1292 - mean_squared_error: 1.1292 - val_loss: 0.3003 - val_mean_squared_error: 0.3003\n",
      "Epoch 44/100\n",
      "396/396 [==============================] - 0s 72us/step - loss: 1.1269 - mean_squared_error: 1.1269 - val_loss: 0.2845 - val_mean_squared_error: 0.2845\n",
      "Epoch 45/100\n",
      "396/396 [==============================] - 0s 69us/step - loss: 1.1244 - mean_squared_error: 1.1244 - val_loss: 0.2808 - val_mean_squared_error: 0.2808\n",
      "Epoch 46/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: 1.1277 - mean_squared_error: 1.1277 - val_loss: 0.2826 - val_mean_squared_error: 0.2826\n",
      "Epoch 47/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 1.1269 - mean_squared_error: 1.1269 - val_loss: 0.2883 - val_mean_squared_error: 0.2883\n",
      "Epoch 48/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: 1.1263 - mean_squared_error: 1.1263 - val_loss: 0.2831 - val_mean_squared_error: 0.2831\n",
      "Epoch 49/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 1.1292 - mean_squared_error: 1.1292 - val_loss: 0.2860 - val_mean_squared_error: 0.2860\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s 71us/step - loss: 1.1251 - mean_squared_error: 1.1251 - val_loss: 0.2907 - val_mean_squared_error: 0.2907\n",
      "Epoch 51/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 1.1263 - mean_squared_error: 1.1263 - val_loss: 0.2797 - val_mean_squared_error: 0.2797\n",
      "Epoch 52/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.1261 - mean_squared_error: 1.1261 - val_loss: 0.2785 - val_mean_squared_error: 0.2785\n",
      "Epoch 53/100\n",
      "396/396 [==============================] - 0s 44us/step - loss: 1.1231 - mean_squared_error: 1.1231 - val_loss: 0.2752 - val_mean_squared_error: 0.2752\n",
      "Epoch 54/100\n",
      "396/396 [==============================] - 0s 79us/step - loss: 1.1223 - mean_squared_error: 1.1223 - val_loss: 0.2941 - val_mean_squared_error: 0.2941\n",
      "Epoch 55/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.1219 - mean_squared_error: 1.1219 - val_loss: 0.2821 - val_mean_squared_error: 0.2821\n",
      "Epoch 56/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.1222 - mean_squared_error: 1.1222 - val_loss: 0.2809 - val_mean_squared_error: 0.2809\n",
      "Epoch 57/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: 1.1225 - mean_squared_error: 1.1225 - val_loss: 0.2942 - val_mean_squared_error: 0.2942\n",
      "Epoch 58/100\n",
      "396/396 [==============================] - 0s 64us/step - loss: 1.1208 - mean_squared_error: 1.1208 - val_loss: 0.2942 - val_mean_squared_error: 0.2942\n",
      "Epoch 59/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 1.1207 - mean_squared_error: 1.1207 - val_loss: 0.3648 - val_mean_squared_error: 0.3648\n",
      "Epoch 60/100\n",
      "396/396 [==============================] - 0s 70us/step - loss: 1.1281 - mean_squared_error: 1.1281 - val_loss: 0.3734 - val_mean_squared_error: 0.3734\n",
      "Epoch 61/100\n",
      "396/396 [==============================] - 0s 69us/step - loss: 1.1295 - mean_squared_error: 1.1295 - val_loss: 0.3295 - val_mean_squared_error: 0.3295\n",
      "Epoch 62/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: 1.1218 - mean_squared_error: 1.1218 - val_loss: 0.2945 - val_mean_squared_error: 0.2945\n",
      "Epoch 63/100\n",
      "396/396 [==============================] - 0s 59us/step - loss: 1.1164 - mean_squared_error: 1.1164 - val_loss: 0.3031 - val_mean_squared_error: 0.3031\n",
      "Epoch 64/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.1191 - mean_squared_error: 1.1191 - val_loss: 0.2996 - val_mean_squared_error: 0.2996\n",
      "Epoch 65/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 1.1187 - mean_squared_error: 1.1187 - val_loss: 0.2898 - val_mean_squared_error: 0.2898\n",
      "Epoch 66/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.1193 - mean_squared_error: 1.1193 - val_loss: 0.2874 - val_mean_squared_error: 0.2874\n",
      "Epoch 67/100\n",
      "396/396 [==============================] - 0s 62us/step - loss: 1.1179 - mean_squared_error: 1.1179 - val_loss: 0.2998 - val_mean_squared_error: 0.2998\n",
      "Epoch 68/100\n",
      "396/396 [==============================] - 0s 70us/step - loss: 1.1163 - mean_squared_error: 1.1163 - val_loss: 0.2975 - val_mean_squared_error: 0.2975\n",
      "Epoch 69/100\n",
      "396/396 [==============================] - 0s 67us/step - loss: 1.1129 - mean_squared_error: 1.1129 - val_loss: 0.3060 - val_mean_squared_error: 0.3060\n",
      "Epoch 70/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: 1.1137 - mean_squared_error: 1.1137 - val_loss: 0.3127 - val_mean_squared_error: 0.3127\n",
      "Epoch 71/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.1156 - mean_squared_error: 1.1156 - val_loss: 0.2941 - val_mean_squared_error: 0.2941\n",
      "Epoch 72/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: 1.1167 - mean_squared_error: 1.1167 - val_loss: 0.2909 - val_mean_squared_error: 0.2909\n",
      "Epoch 73/100\n",
      "396/396 [==============================] - 0s 77us/step - loss: 1.1125 - mean_squared_error: 1.1125 - val_loss: 0.2816 - val_mean_squared_error: 0.2816\n",
      "Epoch 74/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.1133 - mean_squared_error: 1.1133 - val_loss: 0.2865 - val_mean_squared_error: 0.2865\n",
      "Epoch 75/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.1113 - mean_squared_error: 1.1113 - val_loss: 0.3660 - val_mean_squared_error: 0.3660\n",
      "Epoch 76/100\n",
      "396/396 [==============================] - 0s 56us/step - loss: 1.1201 - mean_squared_error: 1.1201 - val_loss: 0.3195 - val_mean_squared_error: 0.3195\n",
      "Epoch 77/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 1.1125 - mean_squared_error: 1.1125 - val_loss: 0.3008 - val_mean_squared_error: 0.3008\n",
      "Epoch 78/100\n",
      "396/396 [==============================] - 0s 70us/step - loss: 1.1106 - mean_squared_error: 1.1106 - val_loss: 0.3094 - val_mean_squared_error: 0.3094\n",
      "Epoch 79/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.1128 - mean_squared_error: 1.1128 - val_loss: 0.3038 - val_mean_squared_error: 0.3038\n",
      "Epoch 80/100\n",
      "396/396 [==============================] - 0s 69us/step - loss: 1.1127 - mean_squared_error: 1.1127 - val_loss: 0.2933 - val_mean_squared_error: 0.2933\n",
      "Epoch 81/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 1.1101 - mean_squared_error: 1.1101 - val_loss: 0.3043 - val_mean_squared_error: 0.3043\n",
      "Epoch 82/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: 1.1087 - mean_squared_error: 1.1087 - val_loss: 0.2921 - val_mean_squared_error: 0.2921\n",
      "Epoch 83/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.1096 - mean_squared_error: 1.1096 - val_loss: 0.3044 - val_mean_squared_error: 0.3044\n",
      "Epoch 84/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.1110 - mean_squared_error: 1.1110 - val_loss: 0.2988 - val_mean_squared_error: 0.2988\n",
      "Epoch 85/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.1069 - mean_squared_error: 1.1069 - val_loss: 0.2890 - val_mean_squared_error: 0.2890\n",
      "Epoch 86/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.1111 - mean_squared_error: 1.1111 - val_loss: 0.2946 - val_mean_squared_error: 0.2946\n",
      "Epoch 87/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 1.1092 - mean_squared_error: 1.1092 - val_loss: 0.2961 - val_mean_squared_error: 0.2961\n",
      "Epoch 88/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.1069 - mean_squared_error: 1.1069 - val_loss: 0.3005 - val_mean_squared_error: 0.3005\n",
      "Epoch 89/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: 1.1069 - mean_squared_error: 1.1069 - val_loss: 0.3106 - val_mean_squared_error: 0.3106\n",
      "Epoch 90/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.1075 - mean_squared_error: 1.1075 - val_loss: 0.2880 - val_mean_squared_error: 0.2880\n",
      "Epoch 91/100\n",
      "396/396 [==============================] - 0s 59us/step - loss: 1.1050 - mean_squared_error: 1.1050 - val_loss: 0.2858 - val_mean_squared_error: 0.2858\n",
      "Epoch 92/100\n",
      "396/396 [==============================] - 0s 45us/step - loss: 1.1062 - mean_squared_error: 1.1062 - val_loss: 0.2993 - val_mean_squared_error: 0.2993\n",
      "Epoch 93/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.1076 - mean_squared_error: 1.1076 - val_loss: 0.2817 - val_mean_squared_error: 0.2817\n",
      "Epoch 94/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.1075 - mean_squared_error: 1.1075 - val_loss: 0.2868 - val_mean_squared_error: 0.2868\n",
      "Epoch 95/100\n",
      "396/396 [==============================] - 0s 43us/step - loss: 1.1069 - mean_squared_error: 1.1069 - val_loss: 0.2961 - val_mean_squared_error: 0.2961\n",
      "Epoch 96/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 1.1039 - mean_squared_error: 1.1039 - val_loss: 0.3079 - val_mean_squared_error: 0.3079\n",
      "Epoch 97/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: 1.1081 - mean_squared_error: 1.1081 - val_loss: 0.3000 - val_mean_squared_error: 0.3000\n",
      "Epoch 98/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: 1.1032 - mean_squared_error: 1.1032 - val_loss: 0.2979 - val_mean_squared_error: 0.2979\n",
      "Epoch 99/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.1052 - mean_squared_error: 1.1052 - val_loss: 0.2998 - val_mean_squared_error: 0.2998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 1.1039 - mean_squared_error: 1.1039 - val_loss: 0.2897 - val_mean_squared_error: 0.2897\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(8, activation='relu', input_dim=12, kernel_initializer='he_normal'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='sgd', loss='mse', metrics=['mse'])\n",
    "\n",
    "he_model = model.fit(X_train, Y_train, batch_size=32, epochs=100,\n",
    "                    verbose=1, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\n",
    "\n",
    "Run the cells below to get training and validation predictions are recalculate our MSE for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0980026908011942\n",
      "0.2896783150613855\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train) \n",
    "print(MSE_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initializer does not really help us to decrease the MSE. We know that initializers can be particularly helpful in deeper networks, and our network isn't very deep. What if we use the `Lecun` initializer with a `tanh` activation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2  Lecun initialization\n",
    "\n",
    "In the cell below, recreate the network again.  This time, set hidden layer's activation to `'tanh'`, and the `kernel_initializer` to `'lecun_normal'`.\n",
    "\n",
    "Then, fit and compile the model as did before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 396 samples, validate on 99 samples\n",
      "Epoch 1/100\n",
      "396/396 [==============================] - 0s 620us/step - loss: 1.5673 - mean_squared_error: 1.5673 - val_loss: 0.3069 - val_mean_squared_error: 0.3069\n",
      "Epoch 2/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.3487 - mean_squared_error: 1.3487 - val_loss: 0.2724 - val_mean_squared_error: 0.2724\n",
      "Epoch 3/100\n",
      "396/396 [==============================] - 0s 82us/step - loss: 1.2831 - mean_squared_error: 1.2831 - val_loss: 0.2597 - val_mean_squared_error: 0.2597\n",
      "Epoch 4/100\n",
      "396/396 [==============================] - 0s 125us/step - loss: 1.2483 - mean_squared_error: 1.2483 - val_loss: 0.2610 - val_mean_squared_error: 0.2610\n",
      "Epoch 5/100\n",
      "396/396 [==============================] - 0s 92us/step - loss: 1.2300 - mean_squared_error: 1.2300 - val_loss: 0.2533 - val_mean_squared_error: 0.2533\n",
      "Epoch 6/100\n",
      "396/396 [==============================] - 0s 75us/step - loss: 1.2140 - mean_squared_error: 1.2140 - val_loss: 0.2574 - val_mean_squared_error: 0.2574\n",
      "Epoch 7/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: 1.2036 - mean_squared_error: 1.2036 - val_loss: 0.2559 - val_mean_squared_error: 0.2559\n",
      "Epoch 8/100\n",
      "396/396 [==============================] - 0s 82us/step - loss: 1.1959 - mean_squared_error: 1.1959 - val_loss: 0.2627 - val_mean_squared_error: 0.2627\n",
      "Epoch 9/100\n",
      "396/396 [==============================] - 0s 69us/step - loss: 1.1908 - mean_squared_error: 1.1908 - val_loss: 0.2684 - val_mean_squared_error: 0.2684\n",
      "Epoch 10/100\n",
      "396/396 [==============================] - 0s 72us/step - loss: 1.1827 - mean_squared_error: 1.1827 - val_loss: 0.2616 - val_mean_squared_error: 0.2616\n",
      "Epoch 11/100\n",
      "396/396 [==============================] - 0s 112us/step - loss: 1.1800 - mean_squared_error: 1.1800 - val_loss: 0.2574 - val_mean_squared_error: 0.2574\n",
      "Epoch 12/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.1742 - mean_squared_error: 1.1742 - val_loss: 0.2550 - val_mean_squared_error: 0.2550\n",
      "Epoch 13/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.1732 - mean_squared_error: 1.1732 - val_loss: 0.2563 - val_mean_squared_error: 0.2563\n",
      "Epoch 14/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.1694 - mean_squared_error: 1.1694 - val_loss: 0.2564 - val_mean_squared_error: 0.2564\n",
      "Epoch 15/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 1.1647 - mean_squared_error: 1.1647 - val_loss: 0.2543 - val_mean_squared_error: 0.2543\n",
      "Epoch 16/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 1.1646 - mean_squared_error: 1.1646 - val_loss: 0.2520 - val_mean_squared_error: 0.2520\n",
      "Epoch 17/100\n",
      "396/396 [==============================] - 0s 59us/step - loss: 1.1620 - mean_squared_error: 1.1620 - val_loss: 0.2575 - val_mean_squared_error: 0.2575\n",
      "Epoch 18/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: 1.1590 - mean_squared_error: 1.1590 - val_loss: 0.2587 - val_mean_squared_error: 0.2587\n",
      "Epoch 19/100\n",
      "396/396 [==============================] - 0s 80us/step - loss: 1.1568 - mean_squared_error: 1.1568 - val_loss: 0.2527 - val_mean_squared_error: 0.2527\n",
      "Epoch 20/100\n",
      "396/396 [==============================] - 0s 67us/step - loss: 1.1561 - mean_squared_error: 1.1561 - val_loss: 0.2516 - val_mean_squared_error: 0.2516\n",
      "Epoch 21/100\n",
      "396/396 [==============================] - 0s 77us/step - loss: 1.1544 - mean_squared_error: 1.1544 - val_loss: 0.2484 - val_mean_squared_error: 0.2484\n",
      "Epoch 22/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 1.1531 - mean_squared_error: 1.1531 - val_loss: 0.2561 - val_mean_squared_error: 0.2561\n",
      "Epoch 23/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: 1.1508 - mean_squared_error: 1.1508 - val_loss: 0.2546 - val_mean_squared_error: 0.2546\n",
      "Epoch 24/100\n",
      "396/396 [==============================] - 0s 80us/step - loss: 1.1499 - mean_squared_error: 1.1499 - val_loss: 0.2511 - val_mean_squared_error: 0.2511\n",
      "Epoch 25/100\n",
      "396/396 [==============================] - 0s 69us/step - loss: 1.1485 - mean_squared_error: 1.1485 - val_loss: 0.2496 - val_mean_squared_error: 0.2496\n",
      "Epoch 26/100\n",
      "396/396 [==============================] - 0s 64us/step - loss: 1.1474 - mean_squared_error: 1.1474 - val_loss: 0.2513 - val_mean_squared_error: 0.2513\n",
      "Epoch 27/100\n",
      "396/396 [==============================] - 0s 97us/step - loss: 1.1459 - mean_squared_error: 1.1459 - val_loss: 0.2541 - val_mean_squared_error: 0.2541\n",
      "Epoch 28/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.1445 - mean_squared_error: 1.1445 - val_loss: 0.2533 - val_mean_squared_error: 0.2533\n",
      "Epoch 29/100\n",
      "396/396 [==============================] - 0s 80us/step - loss: 1.1439 - mean_squared_error: 1.1439 - val_loss: 0.2739 - val_mean_squared_error: 0.2739\n",
      "Epoch 30/100\n",
      "396/396 [==============================] - 0s 69us/step - loss: 1.1437 - mean_squared_error: 1.1437 - val_loss: 0.2664 - val_mean_squared_error: 0.2664\n",
      "Epoch 31/100\n",
      "396/396 [==============================] - 0s 62us/step - loss: 1.1422 - mean_squared_error: 1.1422 - val_loss: 0.2578 - val_mean_squared_error: 0.2578\n",
      "Epoch 32/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.1404 - mean_squared_error: 1.1404 - val_loss: 0.2824 - val_mean_squared_error: 0.2824\n",
      "Epoch 33/100\n",
      "396/396 [==============================] - 0s 82us/step - loss: 1.1428 - mean_squared_error: 1.1428 - val_loss: 0.2692 - val_mean_squared_error: 0.2692\n",
      "Epoch 34/100\n",
      "396/396 [==============================] - 0s 207us/step - loss: 1.1401 - mean_squared_error: 1.1401 - val_loss: 0.2877 - val_mean_squared_error: 0.2877\n",
      "Epoch 35/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: 1.1436 - mean_squared_error: 1.1436 - val_loss: 0.2655 - val_mean_squared_error: 0.2655\n",
      "Epoch 36/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.1393 - mean_squared_error: 1.1393 - val_loss: 0.2561 - val_mean_squared_error: 0.2561\n",
      "Epoch 37/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.1369 - mean_squared_error: 1.1369 - val_loss: 0.2527 - val_mean_squared_error: 0.2527\n",
      "Epoch 38/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 1.1370 - mean_squared_error: 1.1370 - val_loss: 0.2566 - val_mean_squared_error: 0.2566\n",
      "Epoch 39/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 1.1370 - mean_squared_error: 1.1370 - val_loss: 0.2561 - val_mean_squared_error: 0.2561\n",
      "Epoch 40/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.1388 - mean_squared_error: 1.1388 - val_loss: 0.2535 - val_mean_squared_error: 0.2535\n",
      "Epoch 41/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.1366 - mean_squared_error: 1.1366 - val_loss: 0.2638 - val_mean_squared_error: 0.2638\n",
      "Epoch 42/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.1368 - mean_squared_error: 1.1368 - val_loss: 0.2600 - val_mean_squared_error: 0.2600\n",
      "Epoch 43/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.1352 - mean_squared_error: 1.1352 - val_loss: 0.2650 - val_mean_squared_error: 0.2650\n",
      "Epoch 44/100\n",
      "396/396 [==============================] - 0s 82us/step - loss: 1.1355 - mean_squared_error: 1.1355 - val_loss: 0.2548 - val_mean_squared_error: 0.2548\n",
      "Epoch 45/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.1345 - mean_squared_error: 1.1345 - val_loss: 0.2600 - val_mean_squared_error: 0.2600\n",
      "Epoch 46/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.1334 - mean_squared_error: 1.1334 - val_loss: 0.2657 - val_mean_squared_error: 0.2657\n",
      "Epoch 47/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: 1.1343 - mean_squared_error: 1.1343 - val_loss: 0.2593 - val_mean_squared_error: 0.2593\n",
      "Epoch 48/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 1.1325 - mean_squared_error: 1.1325 - val_loss: 0.2543 - val_mean_squared_error: 0.2543\n",
      "Epoch 49/100\n",
      "396/396 [==============================] - 0s 109us/step - loss: 1.1346 - mean_squared_error: 1.1346 - val_loss: 0.2524 - val_mean_squared_error: 0.2524\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s 142us/step - loss: 1.1318 - mean_squared_error: 1.1318 - val_loss: 0.2608 - val_mean_squared_error: 0.2608\n",
      "Epoch 51/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.1327 - mean_squared_error: 1.1327 - val_loss: 0.2577 - val_mean_squared_error: 0.2577\n",
      "Epoch 52/100\n",
      "396/396 [==============================] - 0s 45us/step - loss: 1.1309 - mean_squared_error: 1.1309 - val_loss: 0.2589 - val_mean_squared_error: 0.2589\n",
      "Epoch 53/100\n",
      "396/396 [==============================] - 0s 59us/step - loss: 1.1306 - mean_squared_error: 1.1306 - val_loss: 0.2594 - val_mean_squared_error: 0.2594\n",
      "Epoch 54/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.1320 - mean_squared_error: 1.1320 - val_loss: 0.2547 - val_mean_squared_error: 0.2547\n",
      "Epoch 55/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: 1.1321 - mean_squared_error: 1.1321 - val_loss: 0.2557 - val_mean_squared_error: 0.2557\n",
      "Epoch 56/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: 1.1309 - mean_squared_error: 1.1309 - val_loss: 0.2573 - val_mean_squared_error: 0.2573\n",
      "Epoch 57/100\n",
      "396/396 [==============================] - 0s 56us/step - loss: 1.1315 - mean_squared_error: 1.1315 - val_loss: 0.2568 - val_mean_squared_error: 0.2568\n",
      "Epoch 58/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: 1.1301 - mean_squared_error: 1.1301 - val_loss: 0.2505 - val_mean_squared_error: 0.2505\n",
      "Epoch 59/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.1315 - mean_squared_error: 1.1315 - val_loss: 0.2509 - val_mean_squared_error: 0.2509\n",
      "Epoch 60/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: 1.1311 - mean_squared_error: 1.1311 - val_loss: 0.2552 - val_mean_squared_error: 0.2552\n",
      "Epoch 61/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 1.1307 - mean_squared_error: 1.1307 - val_loss: 0.2548 - val_mean_squared_error: 0.2548\n",
      "Epoch 62/100\n",
      "396/396 [==============================] - 0s 72us/step - loss: 1.1286 - mean_squared_error: 1.1286 - val_loss: 0.2534 - val_mean_squared_error: 0.2534\n",
      "Epoch 63/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.1289 - mean_squared_error: 1.1289 - val_loss: 0.2550 - val_mean_squared_error: 0.2550\n",
      "Epoch 64/100\n",
      "396/396 [==============================] - 0s 43us/step - loss: 1.1290 - mean_squared_error: 1.1290 - val_loss: 0.2599 - val_mean_squared_error: 0.2599\n",
      "Epoch 65/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 1.1298 - mean_squared_error: 1.1298 - val_loss: 0.2533 - val_mean_squared_error: 0.2533\n",
      "Epoch 66/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.1285 - mean_squared_error: 1.1285 - val_loss: 0.2534 - val_mean_squared_error: 0.2534\n",
      "Epoch 67/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 1.1276 - mean_squared_error: 1.1276 - val_loss: 0.2494 - val_mean_squared_error: 0.2494\n",
      "Epoch 68/100\n",
      "396/396 [==============================] - 0s 46us/step - loss: 1.1269 - mean_squared_error: 1.1269 - val_loss: 0.2570 - val_mean_squared_error: 0.2570\n",
      "Epoch 69/100\n",
      "396/396 [==============================] - 0s 74us/step - loss: 1.1271 - mean_squared_error: 1.1271 - val_loss: 0.2556 - val_mean_squared_error: 0.2556\n",
      "Epoch 70/100\n",
      "396/396 [==============================] - 0s 67us/step - loss: 1.1275 - mean_squared_error: 1.1275 - val_loss: 0.2547 - val_mean_squared_error: 0.2547\n",
      "Epoch 71/100\n",
      "396/396 [==============================] - 0s 59us/step - loss: 1.1265 - mean_squared_error: 1.1265 - val_loss: 0.2531 - val_mean_squared_error: 0.2531\n",
      "Epoch 72/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.1268 - mean_squared_error: 1.1268 - val_loss: 0.2553 - val_mean_squared_error: 0.2553\n",
      "Epoch 73/100\n",
      "396/396 [==============================] - 0s 89us/step - loss: 1.1270 - mean_squared_error: 1.1270 - val_loss: 0.2587 - val_mean_squared_error: 0.2587\n",
      "Epoch 74/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.1262 - mean_squared_error: 1.1262 - val_loss: 0.2575 - val_mean_squared_error: 0.2575\n",
      "Epoch 75/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.1250 - mean_squared_error: 1.1250 - val_loss: 0.2633 - val_mean_squared_error: 0.2633\n",
      "Epoch 76/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 1.1258 - mean_squared_error: 1.1258 - val_loss: 0.2834 - val_mean_squared_error: 0.2834\n",
      "Epoch 77/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.1276 - mean_squared_error: 1.1276 - val_loss: 0.2675 - val_mean_squared_error: 0.2675\n",
      "Epoch 78/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.1258 - mean_squared_error: 1.1258 - val_loss: 0.2588 - val_mean_squared_error: 0.2588\n",
      "Epoch 79/100\n",
      "396/396 [==============================] - 0s 74us/step - loss: 1.1256 - mean_squared_error: 1.1256 - val_loss: 0.2523 - val_mean_squared_error: 0.2523\n",
      "Epoch 80/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.1251 - mean_squared_error: 1.1251 - val_loss: 0.2531 - val_mean_squared_error: 0.2531\n",
      "Epoch 81/100\n",
      "396/396 [==============================] - 0s 69us/step - loss: 1.1231 - mean_squared_error: 1.1231 - val_loss: 0.2486 - val_mean_squared_error: 0.2486\n",
      "Epoch 82/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.1249 - mean_squared_error: 1.1249 - val_loss: 0.2534 - val_mean_squared_error: 0.2534\n",
      "Epoch 83/100\n",
      "396/396 [==============================] - 0s 67us/step - loss: 1.1235 - mean_squared_error: 1.1235 - val_loss: 0.2549 - val_mean_squared_error: 0.2549\n",
      "Epoch 84/100\n",
      "396/396 [==============================] - 0s 84us/step - loss: 1.1229 - mean_squared_error: 1.1229 - val_loss: 0.2522 - val_mean_squared_error: 0.2522\n",
      "Epoch 85/100\n",
      "396/396 [==============================] - 0s 56us/step - loss: 1.1232 - mean_squared_error: 1.1232 - val_loss: 0.2607 - val_mean_squared_error: 0.2607\n",
      "Epoch 86/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.1234 - mean_squared_error: 1.1234 - val_loss: 0.2585 - val_mean_squared_error: 0.2585\n",
      "Epoch 87/100\n",
      "396/396 [==============================] - 0s 85us/step - loss: 1.1218 - mean_squared_error: 1.1218 - val_loss: 0.2538 - val_mean_squared_error: 0.2538\n",
      "Epoch 88/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.1225 - mean_squared_error: 1.1225 - val_loss: 0.2609 - val_mean_squared_error: 0.2609\n",
      "Epoch 89/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: 1.1229 - mean_squared_error: 1.1229 - val_loss: 0.2549 - val_mean_squared_error: 0.2549\n",
      "Epoch 90/100\n",
      "396/396 [==============================] - 0s 72us/step - loss: 1.1220 - mean_squared_error: 1.1220 - val_loss: 0.2502 - val_mean_squared_error: 0.2502\n",
      "Epoch 91/100\n",
      "396/396 [==============================] - 0s 56us/step - loss: 1.1215 - mean_squared_error: 1.1215 - val_loss: 0.2832 - val_mean_squared_error: 0.2832\n",
      "Epoch 92/100\n",
      "396/396 [==============================] - ETA: 0s - loss: 1.7629 - mean_squared_error: 1.76 - 0s 57us/step - loss: 1.1244 - mean_squared_error: 1.1244 - val_loss: 0.2590 - val_mean_squared_error: 0.2590\n",
      "Epoch 93/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 1.1226 - mean_squared_error: 1.1226 - val_loss: 0.2547 - val_mean_squared_error: 0.2547\n",
      "Epoch 94/100\n",
      "396/396 [==============================] - 0s 64us/step - loss: 1.1224 - mean_squared_error: 1.1224 - val_loss: 0.2498 - val_mean_squared_error: 0.2498\n",
      "Epoch 95/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.1207 - mean_squared_error: 1.1207 - val_loss: 0.2517 - val_mean_squared_error: 0.2517\n",
      "Epoch 96/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.1211 - mean_squared_error: 1.1211 - val_loss: 0.2568 - val_mean_squared_error: 0.2568\n",
      "Epoch 97/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 1.1198 - mean_squared_error: 1.1198 - val_loss: 0.2501 - val_mean_squared_error: 0.2501\n",
      "Epoch 98/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.1205 - mean_squared_error: 1.1205 - val_loss: 0.2520 - val_mean_squared_error: 0.2520\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s 50us/step - loss: 1.1207 - mean_squared_error: 1.1207 - val_loss: 0.2492 - val_mean_squared_error: 0.2492\n",
      "Epoch 100/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: 1.1204 - mean_squared_error: 1.1204 - val_loss: 0.2580 - val_mean_squared_error: 0.2580\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(8, activation='tanh', input_dim=12, kernel_initializer='lecun_normal'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='sgd', loss='mse', metrics=['mse'])\n",
    "\n",
    "lecun_model = model.fit(X_train, Y_train, batch_size=32, epochs=100,\n",
    "                        verbose=1, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cells below to get the predictions and calculate the MSE for training and validation again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1152763443738078\n",
      "0.2579843313651145\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train) \n",
    "print(MSE_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option we have is to play with the optimizers we choose for gradient descent during our back propagation step.  So far, we've only made use of basic `'sgd'`, or **_Stochastic Gradient Descent_**.  However, there are more advanced optimizers available to use will often converge to better minima, usually in a quicker fashion. \n",
    "\n",
    "In this lab, we'll try the two most popular methods: **_RMSprop_** and **_adam_**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 RMSprop\n",
    "\n",
    "In the cell below, recreate the original network that we built in this lab--no kernel intialization parameter, and the activation set to `'relu'`. \n",
    "\n",
    "This time, when you compile the model, set the `optimizer` parameter to `\"rmsprop\"`.  No changes to the `fit()` call are needed--keep those parameters the same.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 396 samples, validate on 99 samples\n",
      "Epoch 1/100\n",
      "396/396 [==============================] - 0s 857us/step - loss: 1.2056 - mean_squared_error: 1.2056 - val_loss: 0.2944 - val_mean_squared_error: 0.2944\n",
      "Epoch 2/100\n",
      "396/396 [==============================] - 0s 129us/step - loss: 1.1889 - mean_squared_error: 1.1889 - val_loss: 0.2942 - val_mean_squared_error: 0.2942\n",
      "Epoch 3/100\n",
      "396/396 [==============================] - 0s 89us/step - loss: 1.1784 - mean_squared_error: 1.1784 - val_loss: 0.2957 - val_mean_squared_error: 0.2957\n",
      "Epoch 4/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: 1.1703 - mean_squared_error: 1.1703 - val_loss: 0.2949 - val_mean_squared_error: 0.2949\n",
      "Epoch 5/100\n",
      "396/396 [==============================] - 0s 79us/step - loss: 1.1649 - mean_squared_error: 1.1649 - val_loss: 0.2938 - val_mean_squared_error: 0.2938\n",
      "Epoch 6/100\n",
      "396/396 [==============================] - 0s 114us/step - loss: 1.1587 - mean_squared_error: 1.1587 - val_loss: 0.2923 - val_mean_squared_error: 0.2923\n",
      "Epoch 7/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.1549 - mean_squared_error: 1.1549 - val_loss: 0.2933 - val_mean_squared_error: 0.2933\n",
      "Epoch 8/100\n",
      "396/396 [==============================] - 0s 69us/step - loss: 1.1498 - mean_squared_error: 1.1498 - val_loss: 0.2950 - val_mean_squared_error: 0.2950\n",
      "Epoch 9/100\n",
      "396/396 [==============================] - 0s 82us/step - loss: 1.1462 - mean_squared_error: 1.1462 - val_loss: 0.2926 - val_mean_squared_error: 0.2926\n",
      "Epoch 10/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 1.1427 - mean_squared_error: 1.1427 - val_loss: 0.2978 - val_mean_squared_error: 0.2978\n",
      "Epoch 11/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.1383 - mean_squared_error: 1.1383 - val_loss: 0.2958 - val_mean_squared_error: 0.2958\n",
      "Epoch 12/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.1373 - mean_squared_error: 1.1373 - val_loss: 0.2975 - val_mean_squared_error: 0.2975\n",
      "Epoch 13/100\n",
      "396/396 [==============================] - 0s 142us/step - loss: 1.1347 - mean_squared_error: 1.1347 - val_loss: 0.2955 - val_mean_squared_error: 0.2955\n",
      "Epoch 14/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: 1.1319 - mean_squared_error: 1.1319 - val_loss: 0.2936 - val_mean_squared_error: 0.2936\n",
      "Epoch 15/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: 1.1298 - mean_squared_error: 1.1298 - val_loss: 0.2937 - val_mean_squared_error: 0.2937\n",
      "Epoch 16/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.1281 - mean_squared_error: 1.1281 - val_loss: 0.2955 - val_mean_squared_error: 0.2955\n",
      "Epoch 17/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.1266 - mean_squared_error: 1.1266 - val_loss: 0.2936 - val_mean_squared_error: 0.2936\n",
      "Epoch 18/100\n",
      "396/396 [==============================] - 0s 79us/step - loss: 1.1247 - mean_squared_error: 1.1247 - val_loss: 0.2930 - val_mean_squared_error: 0.2930\n",
      "Epoch 19/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.1229 - mean_squared_error: 1.1229 - val_loss: 0.2920 - val_mean_squared_error: 0.2920\n",
      "Epoch 20/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.1212 - mean_squared_error: 1.1212 - val_loss: 0.2903 - val_mean_squared_error: 0.2903\n",
      "Epoch 21/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.1198 - mean_squared_error: 1.1198 - val_loss: 0.2946 - val_mean_squared_error: 0.2946\n",
      "Epoch 22/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 1.1182 - mean_squared_error: 1.1182 - val_loss: 0.2950 - val_mean_squared_error: 0.2950\n",
      "Epoch 23/100\n",
      "396/396 [==============================] - 0s 85us/step - loss: 1.1175 - mean_squared_error: 1.1175 - val_loss: 0.2969 - val_mean_squared_error: 0.2969\n",
      "Epoch 24/100\n",
      "396/396 [==============================] - 0s 64us/step - loss: 1.1152 - mean_squared_error: 1.1152 - val_loss: 0.2960 - val_mean_squared_error: 0.2960\n",
      "Epoch 25/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: 1.1148 - mean_squared_error: 1.1148 - val_loss: 0.2955 - val_mean_squared_error: 0.2955\n",
      "Epoch 26/100\n",
      "396/396 [==============================] - 0s 59us/step - loss: 1.1133 - mean_squared_error: 1.1133 - val_loss: 0.2963 - val_mean_squared_error: 0.2963\n",
      "Epoch 27/100\n",
      "396/396 [==============================] - 0s 74us/step - loss: 1.1114 - mean_squared_error: 1.1114 - val_loss: 0.2921 - val_mean_squared_error: 0.2921\n",
      "Epoch 28/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 1.1111 - mean_squared_error: 1.1111 - val_loss: 0.2912 - val_mean_squared_error: 0.2912\n",
      "Epoch 29/100\n",
      "396/396 [==============================] - 0s 91us/step - loss: 1.1095 - mean_squared_error: 1.1095 - val_loss: 0.2905 - val_mean_squared_error: 0.2905\n",
      "Epoch 30/100\n",
      "396/396 [==============================] - 0s 70us/step - loss: 1.1085 - mean_squared_error: 1.1085 - val_loss: 0.2901 - val_mean_squared_error: 0.2901\n",
      "Epoch 31/100\n",
      "396/396 [==============================] - 0s 175us/step - loss: 1.1072 - mean_squared_error: 1.1072 - val_loss: 0.2898 - val_mean_squared_error: 0.2898\n",
      "Epoch 32/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.1061 - mean_squared_error: 1.1061 - val_loss: 0.2904 - val_mean_squared_error: 0.2904\n",
      "Epoch 33/100\n",
      "396/396 [==============================] - 0s 89us/step - loss: 1.1055 - mean_squared_error: 1.1055 - val_loss: 0.2892 - val_mean_squared_error: 0.2892\n",
      "Epoch 34/100\n",
      "396/396 [==============================] - 0s 77us/step - loss: 1.1038 - mean_squared_error: 1.1038 - val_loss: 0.2905 - val_mean_squared_error: 0.2905\n",
      "Epoch 35/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.1030 - mean_squared_error: 1.1030 - val_loss: 0.2899 - val_mean_squared_error: 0.2899\n",
      "Epoch 36/100\n",
      "396/396 [==============================] - 0s 59us/step - loss: 1.1009 - mean_squared_error: 1.1009 - val_loss: 0.2892 - val_mean_squared_error: 0.2892\n",
      "Epoch 37/100\n",
      "396/396 [==============================] - 0s 208us/step - loss: 1.0999 - mean_squared_error: 1.0999 - val_loss: 0.2890 - val_mean_squared_error: 0.2890\n",
      "Epoch 38/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.0988 - mean_squared_error: 1.0988 - val_loss: 0.2877 - val_mean_squared_error: 0.2877\n",
      "Epoch 39/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: 1.0980 - mean_squared_error: 1.0980 - val_loss: 0.2869 - val_mean_squared_error: 0.2869\n",
      "Epoch 40/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.0966 - mean_squared_error: 1.0966 - val_loss: 0.2863 - val_mean_squared_error: 0.2863\n",
      "Epoch 41/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 1.0954 - mean_squared_error: 1.0954 - val_loss: 0.2865 - val_mean_squared_error: 0.2865\n",
      "Epoch 42/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.0938 - mean_squared_error: 1.0938 - val_loss: 0.2870 - val_mean_squared_error: 0.2870\n",
      "Epoch 43/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 1.0923 - mean_squared_error: 1.0923 - val_loss: 0.2873 - val_mean_squared_error: 0.2873\n",
      "Epoch 44/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: 1.0916 - mean_squared_error: 1.0916 - val_loss: 0.2900 - val_mean_squared_error: 0.2900\n",
      "Epoch 45/100\n",
      "396/396 [==============================] - 0s 56us/step - loss: 1.0888 - mean_squared_error: 1.0888 - val_loss: 0.2895 - val_mean_squared_error: 0.2895\n",
      "Epoch 46/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.0882 - mean_squared_error: 1.0882 - val_loss: 0.2896 - val_mean_squared_error: 0.2896\n",
      "Epoch 47/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.0866 - mean_squared_error: 1.0866 - val_loss: 0.2882 - val_mean_squared_error: 0.2882\n",
      "Epoch 48/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: 1.0863 - mean_squared_error: 1.0863 - val_loss: 0.2858 - val_mean_squared_error: 0.2858\n",
      "Epoch 49/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 1.0839 - mean_squared_error: 1.0839 - val_loss: 0.2860 - val_mean_squared_error: 0.2860\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s 140us/step - loss: 1.0835 - mean_squared_error: 1.0835 - val_loss: 0.2891 - val_mean_squared_error: 0.2891\n",
      "Epoch 51/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 1.0815 - mean_squared_error: 1.0815 - val_loss: 0.2867 - val_mean_squared_error: 0.2867\n",
      "Epoch 52/100\n",
      "396/396 [==============================] - 0s 64us/step - loss: 1.0810 - mean_squared_error: 1.0810 - val_loss: 0.2846 - val_mean_squared_error: 0.2846\n",
      "Epoch 53/100\n",
      "396/396 [==============================] - 0s 64us/step - loss: 1.0802 - mean_squared_error: 1.0802 - val_loss: 0.2858 - val_mean_squared_error: 0.2858\n",
      "Epoch 54/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: 1.0781 - mean_squared_error: 1.0781 - val_loss: 0.2887 - val_mean_squared_error: 0.2887\n",
      "Epoch 55/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 1.0769 - mean_squared_error: 1.0769 - val_loss: 0.2855 - val_mean_squared_error: 0.2855\n",
      "Epoch 56/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: 1.0755 - mean_squared_error: 1.0755 - val_loss: 0.2850 - val_mean_squared_error: 0.2850\n",
      "Epoch 57/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: 1.0749 - mean_squared_error: 1.0749 - val_loss: 0.2829 - val_mean_squared_error: 0.2829\n",
      "Epoch 58/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: 1.0739 - mean_squared_error: 1.0739 - val_loss: 0.2835 - val_mean_squared_error: 0.2835\n",
      "Epoch 59/100\n",
      "396/396 [==============================] - 0s 56us/step - loss: 1.0731 - mean_squared_error: 1.0731 - val_loss: 0.2838 - val_mean_squared_error: 0.2838\n",
      "Epoch 60/100\n",
      "396/396 [==============================] - 0s 97us/step - loss: 1.0719 - mean_squared_error: 1.0719 - val_loss: 0.2835 - val_mean_squared_error: 0.2835\n",
      "Epoch 61/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 1.0710 - mean_squared_error: 1.0710 - val_loss: 0.2839 - val_mean_squared_error: 0.2839\n",
      "Epoch 62/100\n",
      "396/396 [==============================] - 0s 67us/step - loss: 1.0693 - mean_squared_error: 1.0693 - val_loss: 0.2800 - val_mean_squared_error: 0.2800\n",
      "Epoch 63/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 1.0697 - mean_squared_error: 1.0697 - val_loss: 0.2847 - val_mean_squared_error: 0.2847\n",
      "Epoch 64/100\n",
      "396/396 [==============================] - 0s 72us/step - loss: 1.0669 - mean_squared_error: 1.0669 - val_loss: 0.2872 - val_mean_squared_error: 0.2872\n",
      "Epoch 65/100\n",
      "396/396 [==============================] - 0s 59us/step - loss: 1.0670 - mean_squared_error: 1.0670 - val_loss: 0.2859 - val_mean_squared_error: 0.2859\n",
      "Epoch 66/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 1.0655 - mean_squared_error: 1.0655 - val_loss: 0.2832 - val_mean_squared_error: 0.2832\n",
      "Epoch 67/100\n",
      "396/396 [==============================] - 0s 62us/step - loss: 1.0658 - mean_squared_error: 1.0658 - val_loss: 0.2869 - val_mean_squared_error: 0.2869\n",
      "Epoch 68/100\n",
      "396/396 [==============================] - 0s 70us/step - loss: 1.0643 - mean_squared_error: 1.0643 - val_loss: 0.2885 - val_mean_squared_error: 0.2885\n",
      "Epoch 69/100\n",
      "396/396 [==============================] - 0s 62us/step - loss: 1.0628 - mean_squared_error: 1.0628 - val_loss: 0.2868 - val_mean_squared_error: 0.2868\n",
      "Epoch 70/100\n",
      "396/396 [==============================] - 0s 77us/step - loss: 1.0627 - mean_squared_error: 1.0627 - val_loss: 0.2876 - val_mean_squared_error: 0.2876\n",
      "Epoch 71/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.0612 - mean_squared_error: 1.0612 - val_loss: 0.2854 - val_mean_squared_error: 0.2854\n",
      "Epoch 72/100\n",
      "396/396 [==============================] - 0s 70us/step - loss: 1.0615 - mean_squared_error: 1.0615 - val_loss: 0.2874 - val_mean_squared_error: 0.2874\n",
      "Epoch 73/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 1.0599 - mean_squared_error: 1.0599 - val_loss: 0.2892 - val_mean_squared_error: 0.2892\n",
      "Epoch 74/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.0599 - mean_squared_error: 1.0599 - val_loss: 0.2911 - val_mean_squared_error: 0.2911\n",
      "Epoch 75/100\n",
      "396/396 [==============================] - 0s 69us/step - loss: 1.0588 - mean_squared_error: 1.0588 - val_loss: 0.2924 - val_mean_squared_error: 0.2924\n",
      "Epoch 76/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.0574 - mean_squared_error: 1.0574 - val_loss: 0.2937 - val_mean_squared_error: 0.2937\n",
      "Epoch 77/100\n",
      "396/396 [==============================] - 0s 62us/step - loss: 1.0566 - mean_squared_error: 1.0566 - val_loss: 0.2958 - val_mean_squared_error: 0.2958\n",
      "Epoch 78/100\n",
      "396/396 [==============================] - 0s 59us/step - loss: 1.0561 - mean_squared_error: 1.0561 - val_loss: 0.2920 - val_mean_squared_error: 0.2920\n",
      "Epoch 79/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: 1.0582 - mean_squared_error: 1.0582 - val_loss: 0.2924 - val_mean_squared_error: 0.2924\n",
      "Epoch 80/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.0555 - mean_squared_error: 1.0555 - val_loss: 0.2929 - val_mean_squared_error: 0.2929\n",
      "Epoch 81/100\n",
      "396/396 [==============================] - 0s 74us/step - loss: 1.0547 - mean_squared_error: 1.0547 - val_loss: 0.2912 - val_mean_squared_error: 0.2912\n",
      "Epoch 82/100\n",
      "396/396 [==============================] - 0s 56us/step - loss: 1.0540 - mean_squared_error: 1.0540 - val_loss: 0.2948 - val_mean_squared_error: 0.2948\n",
      "Epoch 83/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: 1.0532 - mean_squared_error: 1.0532 - val_loss: 0.2931 - val_mean_squared_error: 0.2931\n",
      "Epoch 84/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 1.0540 - mean_squared_error: 1.0540 - val_loss: 0.2966 - val_mean_squared_error: 0.2966\n",
      "Epoch 85/100\n",
      "396/396 [==============================] - 0s 84us/step - loss: 1.0515 - mean_squared_error: 1.0515 - val_loss: 0.2964 - val_mean_squared_error: 0.2964\n",
      "Epoch 86/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: 1.0514 - mean_squared_error: 1.0514 - val_loss: 0.2929 - val_mean_squared_error: 0.2929\n",
      "Epoch 87/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.0511 - mean_squared_error: 1.0511 - val_loss: 0.2915 - val_mean_squared_error: 0.2915\n",
      "Epoch 88/100\n",
      "396/396 [==============================] - 0s 67us/step - loss: 1.0509 - mean_squared_error: 1.0509 - val_loss: 0.2929 - val_mean_squared_error: 0.2929\n",
      "Epoch 89/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 1.0507 - mean_squared_error: 1.0507 - val_loss: 0.2933 - val_mean_squared_error: 0.2933\n",
      "Epoch 90/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.0496 - mean_squared_error: 1.0496 - val_loss: 0.2952 - val_mean_squared_error: 0.2952\n",
      "Epoch 91/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 1.0503 - mean_squared_error: 1.0503 - val_loss: 0.2938 - val_mean_squared_error: 0.2938\n",
      "Epoch 92/100\n",
      "396/396 [==============================] - 0s 59us/step - loss: 1.0483 - mean_squared_error: 1.0483 - val_loss: 0.2933 - val_mean_squared_error: 0.2933\n",
      "Epoch 93/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.0478 - mean_squared_error: 1.0478 - val_loss: 0.2951 - val_mean_squared_error: 0.2951\n",
      "Epoch 94/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: 1.0485 - mean_squared_error: 1.0485 - val_loss: 0.2973 - val_mean_squared_error: 0.2973\n",
      "Epoch 95/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 1.0463 - mean_squared_error: 1.0463 - val_loss: 0.2941 - val_mean_squared_error: 0.2941\n",
      "Epoch 96/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.0466 - mean_squared_error: 1.0466 - val_loss: 0.2971 - val_mean_squared_error: 0.2971\n",
      "Epoch 97/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.0452 - mean_squared_error: 1.0452 - val_loss: 0.2978 - val_mean_squared_error: 0.2978\n",
      "Epoch 98/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.0447 - mean_squared_error: 1.0447 - val_loss: 0.2964 - val_mean_squared_error: 0.2964\n",
      "Epoch 99/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.0451 - mean_squared_error: 1.0451 - val_loss: 0.2981 - val_mean_squared_error: 0.2981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 1.0430 - mean_squared_error: 1.0430 - val_loss: 0.2957 - val_mean_squared_error: 0.2957\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(8, activation='relu', input_dim=12))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['mse'])\n",
    "\n",
    "rms_model = model.fit(X_train, Y_train, batch_size=32, epochs=100,\n",
    "                      verbose=1, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cell below to get predictions and compute the MSE again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.040166996711317\n",
      "0.29571725642903124\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train) \n",
    "print(MSE_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Adam\n",
    "\n",
    "Another popular optimizer is **_adam_**, which stands for `Adaptive Moment Estimation`. This is an optimzer that was created and open-sourced by a team at OpenAI, and is generally seen as the go-to choice for optimizers today. Adam combines the RMSprop algorithm with the concept of momentum, and is generally very effective at getting converging quickly.  \n",
    "\n",
    "In the cell below, create the same network that we did above, but this time, set the optimizer parameter to `'adam'`.  Leave all other parameters the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 396 samples, validate on 99 samples\n",
      "Epoch 1/100\n",
      "396/396 [==============================] - 0s 978us/step - loss: 1.5154 - mean_squared_error: 1.5154 - val_loss: 0.6113 - val_mean_squared_error: 0.6113\n",
      "Epoch 2/100\n",
      "396/396 [==============================] - 0s 122us/step - loss: 1.4299 - mean_squared_error: 1.4299 - val_loss: 0.5200 - val_mean_squared_error: 0.5200\n",
      "Epoch 3/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.3631 - mean_squared_error: 1.3631 - val_loss: 0.4620 - val_mean_squared_error: 0.4620\n",
      "Epoch 4/100\n",
      "396/396 [==============================] - 0s 56us/step - loss: 1.3189 - mean_squared_error: 1.3189 - val_loss: 0.4145 - val_mean_squared_error: 0.4145\n",
      "Epoch 5/100\n",
      "396/396 [==============================] - 0s 91us/step - loss: 1.2829 - mean_squared_error: 1.2829 - val_loss: 0.3751 - val_mean_squared_error: 0.3751\n",
      "Epoch 6/100\n",
      "396/396 [==============================] - 0s 74us/step - loss: 1.2564 - mean_squared_error: 1.2564 - val_loss: 0.3393 - val_mean_squared_error: 0.3393\n",
      "Epoch 7/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 1.2371 - mean_squared_error: 1.2371 - val_loss: 0.3124 - val_mean_squared_error: 0.3124\n",
      "Epoch 8/100\n",
      "396/396 [==============================] - 0s 109us/step - loss: 1.2216 - mean_squared_error: 1.2216 - val_loss: 0.3049 - val_mean_squared_error: 0.3049\n",
      "Epoch 9/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 1.2110 - mean_squared_error: 1.2110 - val_loss: 0.2906 - val_mean_squared_error: 0.2906\n",
      "Epoch 10/100\n",
      "396/396 [==============================] - 0s 96us/step - loss: 1.2027 - mean_squared_error: 1.2027 - val_loss: 0.2759 - val_mean_squared_error: 0.2759\n",
      "Epoch 11/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 1.1941 - mean_squared_error: 1.1941 - val_loss: 0.2657 - val_mean_squared_error: 0.2657\n",
      "Epoch 12/100\n",
      "396/396 [==============================] - 0s 56us/step - loss: 1.1887 - mean_squared_error: 1.1887 - val_loss: 0.2634 - val_mean_squared_error: 0.2634\n",
      "Epoch 13/100\n",
      "396/396 [==============================] - 0s 94us/step - loss: 1.1836 - mean_squared_error: 1.1836 - val_loss: 0.2575 - val_mean_squared_error: 0.2575\n",
      "Epoch 14/100\n",
      "396/396 [==============================] - 0s 70us/step - loss: 1.1790 - mean_squared_error: 1.1790 - val_loss: 0.2517 - val_mean_squared_error: 0.2517\n",
      "Epoch 15/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.1750 - mean_squared_error: 1.1750 - val_loss: 0.2471 - val_mean_squared_error: 0.2471\n",
      "Epoch 16/100\n",
      "396/396 [==============================] - 0s 90us/step - loss: 1.1714 - mean_squared_error: 1.1714 - val_loss: 0.2416 - val_mean_squared_error: 0.2416\n",
      "Epoch 17/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 1.1689 - mean_squared_error: 1.1689 - val_loss: 0.2391 - val_mean_squared_error: 0.2391\n",
      "Epoch 18/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.1656 - mean_squared_error: 1.1656 - val_loss: 0.2367 - val_mean_squared_error: 0.2367\n",
      "Epoch 19/100\n",
      "396/396 [==============================] - 0s 90us/step - loss: 1.1633 - mean_squared_error: 1.1633 - val_loss: 0.2319 - val_mean_squared_error: 0.2319\n",
      "Epoch 20/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.1606 - mean_squared_error: 1.1606 - val_loss: 0.2276 - val_mean_squared_error: 0.2276\n",
      "Epoch 21/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.1583 - mean_squared_error: 1.1583 - val_loss: 0.2262 - val_mean_squared_error: 0.2262\n",
      "Epoch 22/100\n",
      "396/396 [==============================] - 0s 178us/step - loss: 1.1562 - mean_squared_error: 1.1562 - val_loss: 0.2248 - val_mean_squared_error: 0.2248\n",
      "Epoch 23/100\n",
      "396/396 [==============================] - 0s 64us/step - loss: 1.1542 - mean_squared_error: 1.1542 - val_loss: 0.2278 - val_mean_squared_error: 0.2278\n",
      "Epoch 24/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: 1.1527 - mean_squared_error: 1.1527 - val_loss: 0.2261 - val_mean_squared_error: 0.2261\n",
      "Epoch 25/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.1515 - mean_squared_error: 1.1515 - val_loss: 0.2308 - val_mean_squared_error: 0.2308\n",
      "Epoch 26/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.1487 - mean_squared_error: 1.1487 - val_loss: 0.2265 - val_mean_squared_error: 0.2265\n",
      "Epoch 27/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 1.1473 - mean_squared_error: 1.1473 - val_loss: 0.2209 - val_mean_squared_error: 0.2209\n",
      "Epoch 28/100\n",
      "396/396 [==============================] - 0s 69us/step - loss: 1.1459 - mean_squared_error: 1.1459 - val_loss: 0.2252 - val_mean_squared_error: 0.2252\n",
      "Epoch 29/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: 1.1449 - mean_squared_error: 1.1449 - val_loss: 0.2225 - val_mean_squared_error: 0.2225\n",
      "Epoch 30/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.1426 - mean_squared_error: 1.1426 - val_loss: 0.2257 - val_mean_squared_error: 0.2257\n",
      "Epoch 31/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 1.1410 - mean_squared_error: 1.1410 - val_loss: 0.2253 - val_mean_squared_error: 0.2253\n",
      "Epoch 32/100\n",
      "396/396 [==============================] - 0s 72us/step - loss: 1.1402 - mean_squared_error: 1.1402 - val_loss: 0.2263 - val_mean_squared_error: 0.2263\n",
      "Epoch 33/100\n",
      "396/396 [==============================] - 0s 72us/step - loss: 1.1386 - mean_squared_error: 1.1386 - val_loss: 0.2241 - val_mean_squared_error: 0.2241\n",
      "Epoch 34/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.1371 - mean_squared_error: 1.1371 - val_loss: 0.2231 - val_mean_squared_error: 0.2231\n",
      "Epoch 35/100\n",
      "396/396 [==============================] - 0s 56us/step - loss: 1.1360 - mean_squared_error: 1.1360 - val_loss: 0.2246 - val_mean_squared_error: 0.2246\n",
      "Epoch 36/100\n",
      "396/396 [==============================] - 0s 56us/step - loss: 1.1350 - mean_squared_error: 1.1350 - val_loss: 0.2229 - val_mean_squared_error: 0.2229\n",
      "Epoch 37/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.1336 - mean_squared_error: 1.1336 - val_loss: 0.2254 - val_mean_squared_error: 0.2254\n",
      "Epoch 38/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 1.1321 - mean_squared_error: 1.1321 - val_loss: 0.2278 - val_mean_squared_error: 0.2278\n",
      "Epoch 39/100\n",
      "396/396 [==============================] - 0s 69us/step - loss: 1.1310 - mean_squared_error: 1.1310 - val_loss: 0.2262 - val_mean_squared_error: 0.2262\n",
      "Epoch 40/100\n",
      "396/396 [==============================] - 0s 72us/step - loss: 1.1298 - mean_squared_error: 1.1298 - val_loss: 0.2279 - val_mean_squared_error: 0.2279\n",
      "Epoch 41/100\n",
      "396/396 [==============================] - 0s 77us/step - loss: 1.1285 - mean_squared_error: 1.1285 - val_loss: 0.2288 - val_mean_squared_error: 0.2288\n",
      "Epoch 42/100\n",
      "396/396 [==============================] - 0s 62us/step - loss: 1.1275 - mean_squared_error: 1.1275 - val_loss: 0.2282 - val_mean_squared_error: 0.2282\n",
      "Epoch 43/100\n",
      "396/396 [==============================] - 0s 95us/step - loss: 1.1263 - mean_squared_error: 1.1263 - val_loss: 0.2277 - val_mean_squared_error: 0.2277\n",
      "Epoch 44/100\n",
      "396/396 [==============================] - 0s 77us/step - loss: 1.1262 - mean_squared_error: 1.1262 - val_loss: 0.2230 - val_mean_squared_error: 0.2230\n",
      "Epoch 45/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 1.1252 - mean_squared_error: 1.1252 - val_loss: 0.2299 - val_mean_squared_error: 0.2299\n",
      "Epoch 46/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 1.1235 - mean_squared_error: 1.1235 - val_loss: 0.2294 - val_mean_squared_error: 0.2294\n",
      "Epoch 47/100\n",
      "396/396 [==============================] - 0s 80us/step - loss: 1.1226 - mean_squared_error: 1.1226 - val_loss: 0.2308 - val_mean_squared_error: 0.2308\n",
      "Epoch 48/100\n",
      "396/396 [==============================] - 0s 95us/step - loss: 1.1214 - mean_squared_error: 1.1214 - val_loss: 0.2352 - val_mean_squared_error: 0.2352\n",
      "Epoch 49/100\n",
      "396/396 [==============================] - 0s 77us/step - loss: 1.1207 - mean_squared_error: 1.1207 - val_loss: 0.2373 - val_mean_squared_error: 0.2373\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s 60us/step - loss: 1.1200 - mean_squared_error: 1.1200 - val_loss: 0.2371 - val_mean_squared_error: 0.2371\n",
      "Epoch 51/100\n",
      "396/396 [==============================] - 0s 97us/step - loss: 1.1192 - mean_squared_error: 1.1192 - val_loss: 0.2345 - val_mean_squared_error: 0.2345\n",
      "Epoch 52/100\n",
      "396/396 [==============================] - 0s 77us/step - loss: 1.1188 - mean_squared_error: 1.1188 - val_loss: 0.2398 - val_mean_squared_error: 0.2398\n",
      "Epoch 53/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.1169 - mean_squared_error: 1.1169 - val_loss: 0.2361 - val_mean_squared_error: 0.2361\n",
      "Epoch 54/100\n",
      "396/396 [==============================] - 0s 118us/step - loss: 1.1154 - mean_squared_error: 1.1154 - val_loss: 0.2385 - val_mean_squared_error: 0.2385\n",
      "Epoch 55/100\n",
      "396/396 [==============================] - 0s 202us/step - loss: 1.1148 - mean_squared_error: 1.1148 - val_loss: 0.2364 - val_mean_squared_error: 0.2364\n",
      "Epoch 56/100\n",
      "396/396 [==============================] - 0s 133us/step - loss: 1.1138 - mean_squared_error: 1.1138 - val_loss: 0.2348 - val_mean_squared_error: 0.2348\n",
      "Epoch 57/100\n",
      "396/396 [==============================] - 0s 98us/step - loss: 1.1126 - mean_squared_error: 1.1126 - val_loss: 0.2389 - val_mean_squared_error: 0.2389\n",
      "Epoch 58/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 1.1120 - mean_squared_error: 1.1120 - val_loss: 0.2379 - val_mean_squared_error: 0.2379\n",
      "Epoch 59/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.1107 - mean_squared_error: 1.1107 - val_loss: 0.2380 - val_mean_squared_error: 0.2380\n",
      "Epoch 60/100\n",
      "396/396 [==============================] - 0s 64us/step - loss: 1.1101 - mean_squared_error: 1.1101 - val_loss: 0.2375 - val_mean_squared_error: 0.2375\n",
      "Epoch 61/100\n",
      "396/396 [==============================] - 0s 67us/step - loss: 1.1092 - mean_squared_error: 1.1092 - val_loss: 0.2349 - val_mean_squared_error: 0.2349\n",
      "Epoch 62/100\n",
      "396/396 [==============================] - 0s 74us/step - loss: 1.1080 - mean_squared_error: 1.1080 - val_loss: 0.2402 - val_mean_squared_error: 0.2402\n",
      "Epoch 63/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.1075 - mean_squared_error: 1.1075 - val_loss: 0.2377 - val_mean_squared_error: 0.2377\n",
      "Epoch 64/100\n",
      "396/396 [==============================] - 0s 87us/step - loss: 1.1064 - mean_squared_error: 1.1064 - val_loss: 0.2356 - val_mean_squared_error: 0.2356\n",
      "Epoch 65/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 1.1056 - mean_squared_error: 1.1056 - val_loss: 0.2345 - val_mean_squared_error: 0.2345\n",
      "Epoch 66/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: 1.1055 - mean_squared_error: 1.1055 - val_loss: 0.2353 - val_mean_squared_error: 0.2353\n",
      "Epoch 67/100\n",
      "396/396 [==============================] - ETA: 0s - loss: 7.1669 - mean_squared_error: 7.16 - 0s 73us/step - loss: 1.1044 - mean_squared_error: 1.1044 - val_loss: 0.2345 - val_mean_squared_error: 0.2345\n",
      "Epoch 68/100\n",
      "396/396 [==============================] - 0s 59us/step - loss: 1.1040 - mean_squared_error: 1.1040 - val_loss: 0.2348 - val_mean_squared_error: 0.2348\n",
      "Epoch 69/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.1022 - mean_squared_error: 1.1022 - val_loss: 0.2419 - val_mean_squared_error: 0.2419\n",
      "Epoch 70/100\n",
      "396/396 [==============================] - 0s 77us/step - loss: 1.1018 - mean_squared_error: 1.1018 - val_loss: 0.2401 - val_mean_squared_error: 0.2401\n",
      "Epoch 71/100\n",
      "396/396 [==============================] - 0s 125us/step - loss: 1.1017 - mean_squared_error: 1.1017 - val_loss: 0.2466 - val_mean_squared_error: 0.2466\n",
      "Epoch 72/100\n",
      "396/396 [==============================] - 0s 67us/step - loss: 1.0998 - mean_squared_error: 1.0998 - val_loss: 0.2450 - val_mean_squared_error: 0.2450\n",
      "Epoch 73/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.0999 - mean_squared_error: 1.0999 - val_loss: 0.2504 - val_mean_squared_error: 0.2504\n",
      "Epoch 74/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 1.0987 - mean_squared_error: 1.0987 - val_loss: 0.2470 - val_mean_squared_error: 0.2470\n",
      "Epoch 75/100\n",
      "396/396 [==============================] - 0s 77us/step - loss: 1.0977 - mean_squared_error: 1.0977 - val_loss: 0.2514 - val_mean_squared_error: 0.2514\n",
      "Epoch 76/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 1.0967 - mean_squared_error: 1.0967 - val_loss: 0.2471 - val_mean_squared_error: 0.2471\n",
      "Epoch 77/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 1.0953 - mean_squared_error: 1.0953 - val_loss: 0.2465 - val_mean_squared_error: 0.2465\n",
      "Epoch 78/100\n",
      "396/396 [==============================] - 0s 72us/step - loss: 1.0951 - mean_squared_error: 1.0951 - val_loss: 0.2419 - val_mean_squared_error: 0.2419\n",
      "Epoch 79/100\n",
      "396/396 [==============================] - 0s 70us/step - loss: 1.0933 - mean_squared_error: 1.0933 - val_loss: 0.2462 - val_mean_squared_error: 0.2462\n",
      "Epoch 80/100\n",
      "396/396 [==============================] - 0s 95us/step - loss: 1.0933 - mean_squared_error: 1.0933 - val_loss: 0.2443 - val_mean_squared_error: 0.2443\n",
      "Epoch 81/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.0930 - mean_squared_error: 1.0930 - val_loss: 0.2405 - val_mean_squared_error: 0.2405\n",
      "Epoch 82/100\n",
      "396/396 [==============================] - 0s 207us/step - loss: 1.0927 - mean_squared_error: 1.0927 - val_loss: 0.2532 - val_mean_squared_error: 0.2532\n",
      "Epoch 83/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: 1.0904 - mean_squared_error: 1.0904 - val_loss: 0.2515 - val_mean_squared_error: 0.2515\n",
      "Epoch 84/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: 1.0899 - mean_squared_error: 1.0899 - val_loss: 0.2471 - val_mean_squared_error: 0.2471\n",
      "Epoch 85/100\n",
      "396/396 [==============================] - 0s 75us/step - loss: 1.0889 - mean_squared_error: 1.0889 - val_loss: 0.2457 - val_mean_squared_error: 0.2457\n",
      "Epoch 86/100\n",
      "396/396 [==============================] - 0s 90us/step - loss: 1.0887 - mean_squared_error: 1.0887 - val_loss: 0.2489 - val_mean_squared_error: 0.2489\n",
      "Epoch 87/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.0875 - mean_squared_error: 1.0875 - val_loss: 0.2478 - val_mean_squared_error: 0.2478\n",
      "Epoch 88/100\n",
      "396/396 [==============================] - 0s 98us/step - loss: 1.0872 - mean_squared_error: 1.0872 - val_loss: 0.2478 - val_mean_squared_error: 0.2478\n",
      "Epoch 89/100\n",
      "396/396 [==============================] - 0s 195us/step - loss: 1.0859 - mean_squared_error: 1.0859 - val_loss: 0.2479 - val_mean_squared_error: 0.2479\n",
      "Epoch 90/100\n",
      "396/396 [==============================] - 0s 74us/step - loss: 1.0852 - mean_squared_error: 1.0852 - val_loss: 0.2477 - val_mean_squared_error: 0.2477\n",
      "Epoch 91/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 1.0844 - mean_squared_error: 1.0844 - val_loss: 0.2512 - val_mean_squared_error: 0.2512\n",
      "Epoch 92/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.0841 - mean_squared_error: 1.0841 - val_loss: 0.2556 - val_mean_squared_error: 0.2556\n",
      "Epoch 93/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 1.0827 - mean_squared_error: 1.0827 - val_loss: 0.2531 - val_mean_squared_error: 0.2531\n",
      "Epoch 94/100\n",
      "396/396 [==============================] - 0s 77us/step - loss: 1.0822 - mean_squared_error: 1.0822 - val_loss: 0.2502 - val_mean_squared_error: 0.2502\n",
      "Epoch 95/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.0827 - mean_squared_error: 1.0827 - val_loss: 0.2476 - val_mean_squared_error: 0.2476\n",
      "Epoch 96/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.0816 - mean_squared_error: 1.0816 - val_loss: 0.2551 - val_mean_squared_error: 0.2551\n",
      "Epoch 97/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.0815 - mean_squared_error: 1.0815 - val_loss: 0.2569 - val_mean_squared_error: 0.2569\n",
      "Epoch 98/100\n",
      "396/396 [==============================] - 0s 89us/step - loss: 1.0811 - mean_squared_error: 1.0811 - val_loss: 0.2576 - val_mean_squared_error: 0.2576\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s 86us/step - loss: 1.0799 - mean_squared_error: 1.0799 - val_loss: 0.2590 - val_mean_squared_error: 0.2590\n",
      "Epoch 100/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 1.0797 - mean_squared_error: 1.0797 - val_loss: 0.2530 - val_mean_squared_error: 0.2530\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(8, activation='relu', input_dim=12))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "adam_model = model.fit(X_train, Y_train, batch_size=32, epochs=100,\n",
    "                      verbose=1, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0780708940958597\n",
      "0.25298893950622975\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train) \n",
    "print(MSE_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Learning rate decay with momentum\n",
    "\n",
    "\n",
    "The final item that we'll get practice with in this lab is implementing a **_Learning Rate Decay_** strategy, along with **_Momentum_**.  We'll accomplish this by creating a `SGD` object and setting learning rate, decay, and momentum parameters at initialization.  In this way, we can then pass in the `SGD` object we've initialized to our specificataions during the compile step, rather than just a string representing an off-the-shelf `'SGD'`  optimizer.  \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Create a `SGD` optimizer, which can be found in the `optimizers` module.  \n",
    "    * Set the `lr` parameter to  `0.03`.\n",
    "    * Set the `decay` parameter to `0.0001`\n",
    "    * Set the `momentum` parameter to `0.9`.\n",
    "* Recreate the same network we used during the previous example.  \n",
    "* Set the optimizer parameter during the compile step to the `sgd` object we created. \n",
    "* Fit the model with the same hyperparameters as we used before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 396 samples, validate on 99 samples\n",
      "Epoch 1/100\n",
      "396/396 [==============================] - 0s 907us/step - loss: 1.5011 - mean_squared_error: 1.5011 - val_loss: 0.5075 - val_mean_squared_error: 0.5075\n",
      "Epoch 2/100\n",
      "396/396 [==============================] - 0s 116us/step - loss: 1.2213 - mean_squared_error: 1.2213 - val_loss: 0.2190 - val_mean_squared_error: 0.2190\n",
      "Epoch 3/100\n",
      "396/396 [==============================] - 0s 72us/step - loss: 1.1833 - mean_squared_error: 1.1833 - val_loss: 0.3215 - val_mean_squared_error: 0.3215\n",
      "Epoch 4/100\n",
      "396/396 [==============================] - 0s 67us/step - loss: 1.2030 - mean_squared_error: 1.2030 - val_loss: 0.2034 - val_mean_squared_error: 0.2034\n",
      "Epoch 5/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 1.1737 - mean_squared_error: 1.1737 - val_loss: 0.2153 - val_mean_squared_error: 0.2153\n",
      "Epoch 6/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: 1.1575 - mean_squared_error: 1.1575 - val_loss: 0.2509 - val_mean_squared_error: 0.2509\n",
      "Epoch 7/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.1474 - mean_squared_error: 1.1474 - val_loss: 0.2213 - val_mean_squared_error: 0.2213\n",
      "Epoch 8/100\n",
      "396/396 [==============================] - 0s 90us/step - loss: 1.1349 - mean_squared_error: 1.1349 - val_loss: 0.2629 - val_mean_squared_error: 0.2629\n",
      "Epoch 9/100\n",
      "396/396 [==============================] - 0s 67us/step - loss: 1.1412 - mean_squared_error: 1.1412 - val_loss: 0.2345 - val_mean_squared_error: 0.2345\n",
      "Epoch 10/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.1294 - mean_squared_error: 1.1294 - val_loss: 0.2550 - val_mean_squared_error: 0.2550\n",
      "Epoch 11/100\n",
      "396/396 [==============================] - 0s 97us/step - loss: 1.1293 - mean_squared_error: 1.1293 - val_loss: 0.2640 - val_mean_squared_error: 0.2640\n",
      "Epoch 12/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 1.1240 - mean_squared_error: 1.1240 - val_loss: 0.2113 - val_mean_squared_error: 0.2113\n",
      "Epoch 13/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.1338 - mean_squared_error: 1.1338 - val_loss: 0.2530 - val_mean_squared_error: 0.2530\n",
      "Epoch 14/100\n",
      "396/396 [==============================] - 0s 80us/step - loss: 1.1000 - mean_squared_error: 1.1000 - val_loss: 0.2149 - val_mean_squared_error: 0.2149\n",
      "Epoch 15/100\n",
      "396/396 [==============================] - 0s 64us/step - loss: 1.0975 - mean_squared_error: 1.0975 - val_loss: 0.2441 - val_mean_squared_error: 0.2441\n",
      "Epoch 16/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.0942 - mean_squared_error: 1.0942 - val_loss: 0.3185 - val_mean_squared_error: 0.3185\n",
      "Epoch 17/100\n",
      "396/396 [==============================] - 0s 62us/step - loss: 1.0841 - mean_squared_error: 1.0841 - val_loss: 0.2174 - val_mean_squared_error: 0.2174\n",
      "Epoch 18/100\n",
      "396/396 [==============================] - 0s 107us/step - loss: 1.1120 - mean_squared_error: 1.1120 - val_loss: 0.3225 - val_mean_squared_error: 0.3225\n",
      "Epoch 19/100\n",
      "396/396 [==============================] - 0s 138us/step - loss: 1.0943 - mean_squared_error: 1.0943 - val_loss: 0.2966 - val_mean_squared_error: 0.2966\n",
      "Epoch 20/100\n",
      "396/396 [==============================] - 0s 79us/step - loss: 1.0633 - mean_squared_error: 1.0633 - val_loss: 0.2530 - val_mean_squared_error: 0.2530\n",
      "Epoch 21/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: 1.0887 - mean_squared_error: 1.0887 - val_loss: 0.2351 - val_mean_squared_error: 0.2351\n",
      "Epoch 22/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: 1.1027 - mean_squared_error: 1.1027 - val_loss: 0.2661 - val_mean_squared_error: 0.2661\n",
      "Epoch 23/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.0810 - mean_squared_error: 1.0810 - val_loss: 0.2943 - val_mean_squared_error: 0.2943\n",
      "Epoch 24/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 1.0991 - mean_squared_error: 1.0991 - val_loss: 0.2468 - val_mean_squared_error: 0.2468\n",
      "Epoch 25/100\n",
      "396/396 [==============================] - 0s 72us/step - loss: 1.0706 - mean_squared_error: 1.0706 - val_loss: 0.2548 - val_mean_squared_error: 0.2548\n",
      "Epoch 26/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: 1.0913 - mean_squared_error: 1.0913 - val_loss: 0.2938 - val_mean_squared_error: 0.2938\n",
      "Epoch 27/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.0670 - mean_squared_error: 1.0670 - val_loss: 0.2837 - val_mean_squared_error: 0.2837\n",
      "Epoch 28/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: 1.0755 - mean_squared_error: 1.0755 - val_loss: 0.3586 - val_mean_squared_error: 0.3586\n",
      "Epoch 29/100\n",
      "396/396 [==============================] - 0s 88us/step - loss: 1.0579 - mean_squared_error: 1.0579 - val_loss: 0.3119 - val_mean_squared_error: 0.3119\n",
      "Epoch 30/100\n",
      "396/396 [==============================] - 0s 69us/step - loss: 1.0788 - mean_squared_error: 1.0788 - val_loss: 0.2926 - val_mean_squared_error: 0.2926\n",
      "Epoch 31/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: 1.1018 - mean_squared_error: 1.1018 - val_loss: 0.3125 - val_mean_squared_error: 0.3125\n",
      "Epoch 32/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 1.0837 - mean_squared_error: 1.0837 - val_loss: 0.4248 - val_mean_squared_error: 0.4248\n",
      "Epoch 33/100\n",
      "396/396 [==============================] - 0s 195us/step - loss: 1.1069 - mean_squared_error: 1.1069 - val_loss: 0.3144 - val_mean_squared_error: 0.3144\n",
      "Epoch 34/100\n",
      "396/396 [==============================] - 0s 84us/step - loss: 1.1089 - mean_squared_error: 1.1089 - val_loss: 0.2977 - val_mean_squared_error: 0.2977\n",
      "Epoch 35/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: 1.0735 - mean_squared_error: 1.0735 - val_loss: 0.3362 - val_mean_squared_error: 0.3362\n",
      "Epoch 36/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: 1.0694 - mean_squared_error: 1.0694 - val_loss: 0.2726 - val_mean_squared_error: 0.2726\n",
      "Epoch 37/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.0616 - mean_squared_error: 1.0616 - val_loss: 0.2702 - val_mean_squared_error: 0.2702\n",
      "Epoch 38/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.0686 - mean_squared_error: 1.0686 - val_loss: 0.4536 - val_mean_squared_error: 0.4536\n",
      "Epoch 39/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 1.0787 - mean_squared_error: 1.0787 - val_loss: 0.2770 - val_mean_squared_error: 0.2770\n",
      "Epoch 40/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 1.0862 - mean_squared_error: 1.0862 - val_loss: 0.3192 - val_mean_squared_error: 0.3192\n",
      "Epoch 41/100\n",
      "396/396 [==============================] - 0s 59us/step - loss: 1.1145 - mean_squared_error: 1.1145 - val_loss: 0.3758 - val_mean_squared_error: 0.3758\n",
      "Epoch 42/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: 1.0615 - mean_squared_error: 1.0615 - val_loss: 0.3310 - val_mean_squared_error: 0.3310\n",
      "Epoch 43/100\n",
      "396/396 [==============================] - 0s 62us/step - loss: 1.0500 - mean_squared_error: 1.0500 - val_loss: 0.3161 - val_mean_squared_error: 0.3161\n",
      "Epoch 44/100\n",
      "396/396 [==============================] - 0s 62us/step - loss: 1.0544 - mean_squared_error: 1.0544 - val_loss: 0.3514 - val_mean_squared_error: 0.3514\n",
      "Epoch 45/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 1.0395 - mean_squared_error: 1.0395 - val_loss: 0.3179 - val_mean_squared_error: 0.3179\n",
      "Epoch 46/100\n",
      "396/396 [==============================] - 0s 59us/step - loss: 1.0285 - mean_squared_error: 1.0285 - val_loss: 0.4000 - val_mean_squared_error: 0.4000\n",
      "Epoch 47/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 1.0387 - mean_squared_error: 1.0387 - val_loss: 0.2910 - val_mean_squared_error: 0.2910\n",
      "Epoch 48/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: 1.0380 - mean_squared_error: 1.0380 - val_loss: 0.3662 - val_mean_squared_error: 0.3662\n",
      "Epoch 49/100\n",
      "396/396 [==============================] - 0s 75us/step - loss: 1.0409 - mean_squared_error: 1.0409 - val_loss: 0.2988 - val_mean_squared_error: 0.2988\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s 83us/step - loss: 1.0552 - mean_squared_error: 1.0552 - val_loss: 0.3036 - val_mean_squared_error: 0.3036\n",
      "Epoch 51/100\n",
      "396/396 [==============================] - 0s 90us/step - loss: 1.0245 - mean_squared_error: 1.0245 - val_loss: 0.3741 - val_mean_squared_error: 0.3741\n",
      "Epoch 52/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: 1.0336 - mean_squared_error: 1.0336 - val_loss: 0.3547 - val_mean_squared_error: 0.3547\n",
      "Epoch 53/100\n",
      "396/396 [==============================] - 0s 89us/step - loss: 1.0380 - mean_squared_error: 1.0380 - val_loss: 0.3544 - val_mean_squared_error: 0.3544\n",
      "Epoch 54/100\n",
      "396/396 [==============================] - 0s 59us/step - loss: 1.1798 - mean_squared_error: 1.1798 - val_loss: 0.3197 - val_mean_squared_error: 0.3197\n",
      "Epoch 55/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 1.1056 - mean_squared_error: 1.1056 - val_loss: 0.2804 - val_mean_squared_error: 0.2804\n",
      "Epoch 56/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: 1.0739 - mean_squared_error: 1.0739 - val_loss: 0.2966 - val_mean_squared_error: 0.2966\n",
      "Epoch 57/100\n",
      "396/396 [==============================] - 0s 91us/step - loss: 1.0328 - mean_squared_error: 1.0328 - val_loss: 0.3399 - val_mean_squared_error: 0.3399\n",
      "Epoch 58/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 1.0309 - mean_squared_error: 1.0309 - val_loss: 0.3750 - val_mean_squared_error: 0.3750\n",
      "Epoch 59/100\n",
      "396/396 [==============================] - 0s 69us/step - loss: 1.0152 - mean_squared_error: 1.0152 - val_loss: 0.3284 - val_mean_squared_error: 0.3284\n",
      "Epoch 60/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.0342 - mean_squared_error: 1.0342 - val_loss: 0.3129 - val_mean_squared_error: 0.3129\n",
      "Epoch 61/100\n",
      "396/396 [==============================] - 0s 72us/step - loss: 1.0179 - mean_squared_error: 1.0179 - val_loss: 0.3167 - val_mean_squared_error: 0.3167\n",
      "Epoch 62/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.0193 - mean_squared_error: 1.0193 - val_loss: 0.3283 - val_mean_squared_error: 0.3283\n",
      "Epoch 63/100\n",
      "396/396 [==============================] - 0s 62us/step - loss: 1.0230 - mean_squared_error: 1.0230 - val_loss: 0.3245 - val_mean_squared_error: 0.3245\n",
      "Epoch 64/100\n",
      "396/396 [==============================] - 0s 62us/step - loss: 1.0598 - mean_squared_error: 1.0598 - val_loss: 0.3253 - val_mean_squared_error: 0.3253\n",
      "Epoch 65/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: 1.0448 - mean_squared_error: 1.0448 - val_loss: 0.3493 - val_mean_squared_error: 0.3493\n",
      "Epoch 66/100\n",
      "396/396 [==============================] - 0s 67us/step - loss: 1.0567 - mean_squared_error: 1.0567 - val_loss: 0.3265 - val_mean_squared_error: 0.3265\n",
      "Epoch 67/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 1.0383 - mean_squared_error: 1.0383 - val_loss: 0.3398 - val_mean_squared_error: 0.3398\n",
      "Epoch 68/100\n",
      "396/396 [==============================] - 0s 46us/step - loss: 1.0167 - mean_squared_error: 1.0167 - val_loss: 0.4173 - val_mean_squared_error: 0.4173\n",
      "Epoch 69/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.0609 - mean_squared_error: 1.0609 - val_loss: 0.3367 - val_mean_squared_error: 0.3367\n",
      "Epoch 70/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 1.0035 - mean_squared_error: 1.0035 - val_loss: 0.3578 - val_mean_squared_error: 0.3578\n",
      "Epoch 71/100\n",
      "396/396 [==============================] - 0s 46us/step - loss: 1.0261 - mean_squared_error: 1.0261 - val_loss: 0.2960 - val_mean_squared_error: 0.2960\n",
      "Epoch 72/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.2162 - mean_squared_error: 1.2162 - val_loss: 0.4119 - val_mean_squared_error: 0.4119\n",
      "Epoch 73/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 1.1746 - mean_squared_error: 1.1746 - val_loss: 0.2882 - val_mean_squared_error: 0.2882\n",
      "Epoch 74/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.0859 - mean_squared_error: 1.0859 - val_loss: 0.4357 - val_mean_squared_error: 0.4357\n",
      "Epoch 75/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: 1.0955 - mean_squared_error: 1.0955 - val_loss: 0.3268 - val_mean_squared_error: 0.3268\n",
      "Epoch 76/100\n",
      "396/396 [==============================] - 0s 92us/step - loss: 1.0564 - mean_squared_error: 1.0564 - val_loss: 0.3346 - val_mean_squared_error: 0.3346\n",
      "Epoch 77/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.0542 - mean_squared_error: 1.0542 - val_loss: 0.3721 - val_mean_squared_error: 0.3721\n",
      "Epoch 78/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: 1.0406 - mean_squared_error: 1.0406 - val_loss: 0.3431 - val_mean_squared_error: 0.3431\n",
      "Epoch 79/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: 1.0857 - mean_squared_error: 1.0857 - val_loss: 0.3837 - val_mean_squared_error: 0.3837\n",
      "Epoch 80/100\n",
      "396/396 [==============================] - 0s 59us/step - loss: 1.1114 - mean_squared_error: 1.1114 - val_loss: 0.3502 - val_mean_squared_error: 0.3502\n",
      "Epoch 81/100\n",
      "396/396 [==============================] - 0s 62us/step - loss: 1.0925 - mean_squared_error: 1.0925 - val_loss: 0.3640 - val_mean_squared_error: 0.3640\n",
      "Epoch 82/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 1.0392 - mean_squared_error: 1.0392 - val_loss: 0.2929 - val_mean_squared_error: 0.2929\n",
      "Epoch 83/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: 1.0166 - mean_squared_error: 1.0166 - val_loss: 0.3075 - val_mean_squared_error: 0.3075\n",
      "Epoch 84/100\n",
      "396/396 [==============================] - 0s 80us/step - loss: 1.0279 - mean_squared_error: 1.0279 - val_loss: 0.3305 - val_mean_squared_error: 0.3305\n",
      "Epoch 85/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: 1.0185 - mean_squared_error: 1.0185 - val_loss: 0.3291 - val_mean_squared_error: 0.3291\n",
      "Epoch 86/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.0194 - mean_squared_error: 1.0194 - val_loss: 0.3091 - val_mean_squared_error: 0.3091\n",
      "Epoch 87/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: 1.0063 - mean_squared_error: 1.0063 - val_loss: 0.3070 - val_mean_squared_error: 0.3070\n",
      "Epoch 88/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 0.9962 - mean_squared_error: 0.9962 - val_loss: 0.3694 - val_mean_squared_error: 0.3694\n",
      "Epoch 89/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.0068 - mean_squared_error: 1.0068 - val_loss: 0.3964 - val_mean_squared_error: 0.3964\n",
      "Epoch 90/100\n",
      "396/396 [==============================] - 0s 67us/step - loss: 1.3642 - mean_squared_error: 1.3642 - val_loss: 0.3857 - val_mean_squared_error: 0.3857\n",
      "Epoch 91/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.1375 - mean_squared_error: 1.1375 - val_loss: 0.3817 - val_mean_squared_error: 0.3817\n",
      "Epoch 92/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: 1.0861 - mean_squared_error: 1.0861 - val_loss: 0.3500 - val_mean_squared_error: 0.3500\n",
      "Epoch 93/100\n",
      "396/396 [==============================] - 0s 72us/step - loss: 1.0697 - mean_squared_error: 1.0697 - val_loss: 0.3594 - val_mean_squared_error: 0.3594\n",
      "Epoch 94/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.0517 - mean_squared_error: 1.0517 - val_loss: 0.3252 - val_mean_squared_error: 0.3252\n",
      "Epoch 95/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: 1.0430 - mean_squared_error: 1.0430 - val_loss: 0.3361 - val_mean_squared_error: 0.3361\n",
      "Epoch 96/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: 1.0538 - mean_squared_error: 1.0538 - val_loss: 0.3124 - val_mean_squared_error: 0.3124\n",
      "Epoch 97/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: 1.0466 - mean_squared_error: 1.0466 - val_loss: 0.3623 - val_mean_squared_error: 0.3623\n",
      "Epoch 98/100\n",
      "396/396 [==============================] - 0s 64us/step - loss: 1.0310 - mean_squared_error: 1.0310 - val_loss: 0.2924 - val_mean_squared_error: 0.2924\n",
      "Epoch 99/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.0396 - mean_squared_error: 1.0396 - val_loss: 0.3349 - val_mean_squared_error: 0.3349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: 1.0309 - mean_squared_error: 1.0309 - val_loss: 0.3295 - val_mean_squared_error: 0.3295\n"
     ]
    }
   ],
   "source": [
    "sgd = optimizers.SGD(lr=0.03, decay=0.0001, momentum=0.9)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(8, activation='relu', input_dim=12))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(optimizer=sgd, loss='mse', metrics=['mse'])\n",
    "\n",
    "decay_model = model.fit(X_train, Y_train, batch_size=32, epochs=100,\n",
    "                      verbose=1, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the cell below to calcluate the MSE for our final version of this model and see how a learning rate decay strategy affected the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99123871467396\n",
      "0.3294581818726523\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train) \n",
    "print(MSE_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "\n",
    "https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "\n",
    "https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n",
    "\n",
    "https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
